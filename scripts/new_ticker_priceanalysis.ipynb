{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6550a2cb-cd1d-435e-b0b3-10b95b1266c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ================== USER SETTINGS ==================\n",
    "API_KEY = \"c5PobUQjaaMTHySILWqmWi9uyIDqYJBi\"\n",
    "\n",
    "SINGLE_DAY_MODE = False        # True -> from=to=TARGET_DATE; False -> use DATE_FROM/DATE_TO\n",
    "TARGET_DATE = \"2025-10-07\"    # YYYY-MM-DD\n",
    "\n",
    "# If SINGLE_DAY_MODE is False, set these:\n",
    "DATE_FROM = \"2016-01-01\"\n",
    "DATE_TO   = \"2025-10-08\"\n",
    "# ==================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf394dd-2a15-4645-a622-158d2778bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "tickers_duck = con.execute(\"SELECT ticker FROM dim_ticker\").fetchall()\n",
    "tickers = [t[0] for t in tickers_duck]\n",
    "TICKERS: List[str] = tickers \n",
    "con.close()\n",
    "\n",
    "print(len(tickers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5944b2-1ddd-4fda-aeaf-968ea9da766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Batch 1/1: 7 symbols\n",
      "\n",
      "Done. Tickers requested: 7\n",
      "Tickers with rows returned: 7\n",
      "Rows fetched: 12259\n",
      "\n",
      "Head:\n",
      "         date ticker        open        high         low   close  adjClose  \\\n",
      "0  2016-01-04   BF-B   39.010000   39.160000   38.000000   38.88     32.81   \n",
      "1  2016-01-04  BRK-B  130.160004  131.029999  128.759995  130.75    130.75   \n",
      "2  2016-01-04    EME   47.280000   47.570000   46.060000   46.25     44.36   \n",
      "3  2016-01-04   IBKR   10.670000   10.790000   10.560000   10.61      9.93   \n",
      "4  2016-01-05   BF-B   38.830000   39.020000   38.480000   38.90     32.83   \n",
      "\n",
      "    volume  \n",
      "0  3212870  \n",
      "1  6869100  \n",
      "2   318400  \n",
      "3  3920020  \n",
      "4  2395675  \n"
     ]
    }
   ],
   "source": [
    "# ✅ Corrected ticker list and variable name\n",
    "tickers = ['CELH', '']\n",
    "TICKERS: list[str] = tickers\n",
    "\n",
    "print(len(tickers))\n",
    "\n",
    "\"\"\"\n",
    "Daily OHLCV downloader (FMP) — concurrent, batched, with retries\n",
    "- Reads from a Python list of tickers (or optional CSV)\n",
    "- Fetches daily bars for a single date (from=to=TARGET_DATE) or a range\n",
    "- Concurrency capped to ~4 in-flight requests (safe for FMP Starter)\n",
    "- Short pauses between batches\n",
    "- Retries 429/5xx with exponential backoff\n",
    "- Prints a summary of tickers with no data for the requested day\n",
    "- Produces a DataFrame `data` (no files written)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ---- Tuning (fast but generally safe for FMP Starter) -------------------------\n",
    "# Parallel workers: FMP commonly allows ~4 parallel requests safely.\n",
    "MAX_WORKERS = 4\n",
    "# Batch size: how many symbols to schedule per wave\n",
    "BATCH_SIZE = 100\n",
    "# Pause between batches (seconds). Keep small to speed up end-to-end.\n",
    "SLEEP_BETWEEN_BATCHES = 2.0\n",
    "# Per-request connect/read timeout\n",
    "REQUEST_TIMEOUT = 20\n",
    "# Max retries for 429/5xx\n",
    "MAX_RETRIES = 3\n",
    "# Backoff base (seconds) for 429/5xx\n",
    "BACKOFF_BASE = 1.5\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _daterange() -> Tuple[str, str]:\n",
    "    if SINGLE_DAY_MODE:\n",
    "        return TARGET_DATE, TARGET_DATE\n",
    "    return DATE_FROM, DATE_TO\n",
    "\n",
    "def fetch_daily(symbol: str, date_from: str, date_to: str) -> Optional[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Fetch daily bars for a symbol over [date_from, date_to].\n",
    "    Retries on 429 and transient 5xx.\n",
    "    Returns list of bar dicts or None on hard failure.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{symbol}\"\n",
    "    params = {\"from\": date_from, \"to\": date_to, \"apikey\": API_KEY}\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        except requests.RequestException:\n",
    "            # transient network error — backoff & retry\n",
    "            if attempt < MAX_RETRIES:\n",
    "                time.sleep(BACKOFF_BASE ** attempt)\n",
    "                continue\n",
    "            return None\n",
    "\n",
    "        # Handle rate limiting / transient server errors\n",
    "        if r.status_code in (429, 502, 503, 504):\n",
    "            if attempt < MAX_RETRIES:\n",
    "                # Try to respect Retry-After if present\n",
    "                retry_after = r.headers.get(\"Retry-After\")\n",
    "                delay = float(retry_after) if retry_after else (BACKOFF_BASE ** attempt)\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            return None\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            # Hard failure; don't retry further\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "        return js.get(\"historical\", [])\n",
    "\n",
    "    return None  # unreachable, but explicit\n",
    "\n",
    "def chunked(lst: List[str], n: int):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "date_from, date_to = _daterange()\n",
    "all_rows: List[Dict] = []\n",
    "no_data: List[str] = []       # symbols that returned zero rows in the requested window\n",
    "hard_fail: List[str] = []     # symbols that errored out after retries\n",
    "\n",
    "total = len(TICKERS)\n",
    "batches = list(chunked(TICKERS, BATCH_SIZE))\n",
    "\n",
    "for bi, batch in enumerate(batches, start=1):\n",
    "    print(f\"Batch {bi}/{len(batches)}: {len(batch)} symbols\")\n",
    "    futures = {}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        for sym in batch:\n",
    "            futures[ex.submit(fetch_daily, sym, date_from, date_to)] = sym\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            sym = futures[fut]\n",
    "            hist = fut.result()\n",
    "            if hist is None:\n",
    "                hard_fail.append(sym)\n",
    "                continue\n",
    "\n",
    "            # Keep only rows that match the exact day in single-day mode\n",
    "            if SINGLE_DAY_MODE:\n",
    "                hist = [h for h in hist if h.get(\"date\") == TARGET_DATE]\n",
    "\n",
    "            if not hist:\n",
    "                no_data.append(sym)\n",
    "                continue\n",
    "\n",
    "            for h in hist:\n",
    "                all_rows.append({\n",
    "                    \"date\": h.get(\"date\"),\n",
    "                    \"ticker\": sym,\n",
    "                    \"open\": h.get(\"open\"),\n",
    "                    \"high\": h.get(\"high\"),\n",
    "                    \"low\": h.get(\"low\"),\n",
    "                    \"close\": h.get(\"close\"),\n",
    "                    \"adjClose\": h.get(\"adjClose\"),\n",
    "                    \"volume\": h.get(\"volume\"),\n",
    "                })\n",
    "\n",
    "    if bi < len(batches):\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "# Build final DataFrame\n",
    "data = pd.DataFrame(all_rows)\n",
    "if not data.empty:\n",
    "    data = data.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "# Progress / diagnostics\n",
    "fetched = data[\"ticker\"].nunique() if not data.empty else 0\n",
    "print(f\"\\nDone. Tickers requested: {total}\")\n",
    "print(f\"Tickers with rows returned: {fetched}\")\n",
    "print(f\"Rows fetched: {len(data)}\")\n",
    "\n",
    "if no_data:\n",
    "    print(\"\\nNo rows returned for the requested date/window:\")\n",
    "    # Show a few, then count\n",
    "    preview = \", \".join(no_data[:20])\n",
    "    more = f\" ... (+{len(no_data)-20} more)\" if len(no_data) > 20 else \"\"\n",
    "    print(preview + more)\n",
    "\n",
    "if hard_fail:\n",
    "    print(\"\\nFailed after retries (HTTP/network errors):\")\n",
    "    preview = \", \".join(hard_fail[:20])\n",
    "    more = f\" ... (+{len(hard_fail)-20} more)\" if len(hard_fail) > 20 else \"\"\n",
    "    print(preview + more)\n",
    "\n",
    "# Show head for quick inspection\n",
    "print(\"\\nHead:\")\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bb1b65-cbae-4a84-be29-e3ea094c0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tickers inserted into dim_ticker: 7\n",
      "Price rows upserted: 12259\n",
      "Metric rows upserted: 583907\n",
      "Snapshot rows now: 25994\n",
      "\n",
      "Sample prices:    ticker_id         dt   open   high    low  close  adj_close   volume\n",
      "0        515 2025-10-07  26.39  26.50  25.18  25.25      25.25  1904828\n",
      "1        515 2025-10-06  26.00  26.58  25.85  26.27      26.27  3511791\n",
      "2        515 2025-10-03  25.35  26.35  25.27  25.95      25.95  3006900\n",
      "3        515 2025-10-02  25.02  25.28  24.00  25.15      25.15  2401941\n",
      "4        515 2025-10-01  23.61  25.24  23.56  25.16      25.16  1894913\n",
      "\n",
      "Sample metrics:    ticker_id         dt  metric_id         value\n",
      "0        515 2025-10-07          8  2.407260e-02\n",
      "1        515 2025-10-07         28 -5.080838e-01\n",
      "2        515 2025-10-07         22  4.134547e+07\n",
      "3        515 2025-10-07         26  1.565875e-02\n",
      "4        515 2025-10-07         19  3.379948e-02\n"
     ]
    }
   ],
   "source": [
    "# === Backfill New Tickers: add to dim_ticker, load 10y OHLCV, compute + write analytics ===\n",
    "import math, numpy as np, pandas as pd, duckdb\n",
    "from datetime import datetime\n",
    "\n",
    "# ================== USER INPUTS ==================\n",
    "DB_PATH = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "# 1) List the tickers you just added/downloaded (exact symbols as in `data['ticker']`)\n",
    "NEW_TICKERS = tickers      # <-- EDIT\n",
    "TICKER_TYPE = \"Stock\"             # per your note, all are Stock\n",
    "\n",
    "# 2) Your downloaded OHLCV is expected in a DataFrame called `data`\n",
    "#    Must include at least: ticker, date, open, high, low, close, adjClose, volume\n",
    "assert \"data\" in globals(), \"Please ensure your downloaded DataFrame is available as `data`.\"\n",
    "\n",
    "# ================== NORMALIZE DOWNLOADED DATA ==================\n",
    "df_raw = data.copy()\n",
    "# Standardize column names\n",
    "df_raw = df_raw.rename(columns={\"adjClose\":\"adj_close\", \"date\":\"dt\"})\n",
    "# Keep the essential set; ignore extra columns if present\n",
    "required = [\"ticker\",\"dt\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]\n",
    "missing = [c for c in required if c not in df_raw.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Downloaded `data` is missing required columns: {missing}\")\n",
    "\n",
    "# Coerce types\n",
    "df_raw[\"ticker\"] = df_raw[\"ticker\"].astype(str).str.upper()\n",
    "df_raw[\"dt\"] = pd.to_datetime(df_raw[\"dt\"]).dt.date\n",
    "for c in [\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]:\n",
    "    df_raw[c] = pd.to_numeric(df_raw[c], errors=\"coerce\")\n",
    "\n",
    "# Only keep the new tickers we care about (defensive)\n",
    "df_raw = df_raw[df_raw[\"ticker\"].isin([t.upper() for t in NEW_TICKERS])].copy()\n",
    "if df_raw.empty:\n",
    "    raise ValueError(\"No rows in `data` match NEW_TICKERS. Check symbols or inputs.\")\n",
    "\n",
    "# ================== CONNECT (write-enabled) ==================\n",
    "con = duckdb.connect(DB_PATH)  # NOT read_only\n",
    "\n",
    "# Ensure dim_ticker exists minimally (optional safety)\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dim_ticker (\n",
    "  ticker_id   INTEGER PRIMARY KEY,\n",
    "  ticker      VARCHAR NOT NULL UNIQUE,\n",
    "  ticker_type VARCHAR\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# ================== 1) INSERT NEW TICKERS INTO dim_ticker ==================\n",
    "# Fetch existing tickers/ids\n",
    "dim_t = con.execute(\"SELECT ticker_id, UPPER(ticker) AS ticker FROM dim_ticker\").df()\n",
    "existing = set(dim_t[\"ticker\"]) if not dim_t.empty else set()\n",
    "\n",
    "incoming = pd.DataFrame({\"ticker\":[t.upper() for t in NEW_TICKERS]})\n",
    "incoming = incoming.drop_duplicates(ignore_index=True)\n",
    "to_insert = incoming[~incoming[\"ticker\"].isin(existing)].copy()\n",
    "\n",
    "if not to_insert.empty:\n",
    "    # Determine next IDs\n",
    "    max_id = con.execute(\"SELECT COALESCE(MAX(ticker_id), 0) FROM dim_ticker\").fetchone()[0]\n",
    "    to_insert[\"ticker_id\"] = [max_id + i + 1 for i in range(len(to_insert))]\n",
    "    to_insert[\"ticker_type\"] = TICKER_TYPE\n",
    "\n",
    "    # Write\n",
    "    con.register(\"df_new_dim_ticker\", to_insert[[\"ticker_id\",\"ticker\",\"ticker_type\"]])\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO dim_ticker (ticker_id, ticker, ticker_type)\n",
    "        SELECT ticker_id, ticker, ticker_type\n",
    "        FROM df_new_dim_ticker;\n",
    "    \"\"\")\n",
    "else:\n",
    "    # Nothing to insert\n",
    "    pass\n",
    "\n",
    "# Refresh map\n",
    "dim_t = con.execute(\"SELECT ticker_id, UPPER(ticker) AS ticker FROM dim_ticker\").df()\n",
    "ticker_map = dict(zip(dim_t[\"ticker\"], dim_t[\"ticker_id\"]))\n",
    "\n",
    "# Map ticker_id into price DataFrame\n",
    "df_prices = df_raw.copy()\n",
    "df_prices[\"ticker_id\"] = df_prices[\"ticker\"].map(ticker_map)\n",
    "if df_prices[\"ticker_id\"].isna().any():\n",
    "    missing_syms = sorted(df_prices.loc[df_prices[\"ticker_id\"].isna(),\"ticker\"].unique().tolist())\n",
    "    raise ValueError(f\"These tickers do not exist in dim_ticker and could not be mapped: {missing_syms}\")\n",
    "\n",
    "# ================== 2) UPSERT PRICES INTO fact_price_daily ==================\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_price_daily (\n",
    "  ticker_id INTEGER NOT NULL,\n",
    "  dt        DATE    NOT NULL,\n",
    "  open      DOUBLE,\n",
    "  high      DOUBLE,\n",
    "  low       DOUBLE,\n",
    "  close     DOUBLE,\n",
    "  adj_close DOUBLE,\n",
    "  volume    DOUBLE,\n",
    "  PRIMARY KEY (ticker_id, dt)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "con.register(\"df_price_src\", df_prices[[\"ticker_id\",\"dt\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]])\n",
    "con.execute(\"\"\"\n",
    "BEGIN TRANSACTION;\n",
    "MERGE INTO fact_price_daily AS t\n",
    "USING df_price_src AS s\n",
    "ON t.ticker_id = s.ticker_id AND t.dt = s.dt\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  open = s.open, high = s.high, low = s.low,\n",
    "  close = s.close, adj_close = s.adj_close, volume = s.volume\n",
    "WHEN NOT MATCHED THEN INSERT (ticker_id, dt, open, high, low, close, adj_close, volume)\n",
    "VALUES (s.ticker_id, s.dt, s.open, s.high, s.low, s.close, s.adj_close, s.volume);\n",
    "COMMIT;\n",
    "\"\"\")\n",
    "\n",
    "# ================== 3) COMPUTE ANALYTICS FOR NEW TICKERS (ALL DATES) ==================\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "WIN_05=5; WIN_10=10; WIN_15=15; WIN_20=20; WIN_50=50; WIN_60=60; WIN_100=100; WIN_200=200; WIN_252=252; WIN_300=300; WIN_750=750\n",
    "SMA_POS_LEN = 3\n",
    "\n",
    "def _ols_slope(arr: np.ndarray) -> float:\n",
    "    m = np.isfinite(arr)\n",
    "    y = arr[m]\n",
    "    n = y.size\n",
    "    if n < 2: return np.nan\n",
    "    x = np.arange(n, dtype=float)\n",
    "    xm, ym = x.mean(), y.mean()\n",
    "    denom = np.sum((x-xm)**2)\n",
    "    if denom == 0: return np.nan\n",
    "    return float(np.sum((x-xm)*(y-ym))/denom)\n",
    "\n",
    "def _max_dd_only(a: np.ndarray) -> float:\n",
    "    a = np.asarray(a, float)\n",
    "    if not np.isfinite(a).any(): return np.nan\n",
    "    # start at first finite positive\n",
    "    peak = np.nan; started=False\n",
    "    best_dd=0.0\n",
    "    for v in a:\n",
    "        if not math.isfinite(v) or v<=0: continue\n",
    "        if not started:\n",
    "            peak=v; started=True; continue\n",
    "        if v>peak: peak=v\n",
    "        dd = v/peak - 1.0\n",
    "        if dd < best_dd: best_dd = dd\n",
    "    return float(best_dd) if started else np.nan\n",
    "\n",
    "def _max_dd_dur_only(a: np.ndarray) -> float:\n",
    "    a = np.asarray(a, float)\n",
    "    if not np.isfinite(a).any(): return np.nan\n",
    "    # duration since peak to trough (index distance)\n",
    "    peak = -1; peak_val = -np.inf\n",
    "    best_dd=0.0; best_dur=0; cur_peak_idx=None\n",
    "    for i,v in enumerate(a):\n",
    "        if not math.isfinite(v) or v<=0: continue\n",
    "        if v>peak_val:\n",
    "            peak_val=v; cur_peak_idx=i\n",
    "        dd = v/peak_val - 1.0\n",
    "        if dd < best_dd:\n",
    "            best_dd = dd\n",
    "            best_dur = i - cur_peak_idx\n",
    "    return float(best_dur)\n",
    "\n",
    "# Pull just the new tickers' full price history from fact_price_daily (to guarantee continuity/order)\n",
    "new_ids = tuple(sorted(set(df_prices[\"ticker_id\"].tolist())))\n",
    "hist = con.execute(f\"\"\"\n",
    "SELECT t.ticker_id, d.ticker, f.dt::DATE AS dt,\n",
    "       f.open, f.high, f.low, f.close, f.adj_close, f.volume\n",
    "FROM fact_price_daily f\n",
    "JOIN dim_ticker d USING (ticker_id)\n",
    "JOIN (SELECT UNNEST([{\",\".join(map(str,new_ids))}]) AS ticker_id) t USING (ticker_id)\n",
    "ORDER BY ticker_id, dt\n",
    "\"\"\").df()\n",
    "\n",
    "# Compute metrics per ticker across ALL dates\n",
    "def compute_metrics(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.sort_values(\"dt\").copy()\n",
    "    close = g[\"adj_close\"].astype(float)\n",
    "    open_ = g[\"open\"].astype(float)\n",
    "    high  = g[\"high\"].astype(float)\n",
    "    low   = g[\"low\"].astype(float)\n",
    "    vol   = pd.to_numeric(g[\"volume\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "    # logs & returns\n",
    "    close_safe = close.replace(0, np.nan)\n",
    "    lp = np.log(close_safe)\n",
    "    prev = close_safe.shift(1)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        lr = np.where((close_safe>0) & (prev>0), np.log(close_safe/prev), np.nan)\n",
    "    lr = pd.Series(lr, index=g.index)\n",
    "    if len(g)>0 and pd.isna(lr.iloc[0]): lr.iloc[0]=0.0\n",
    "\n",
    "    # MAs\n",
    "    ma20  = close.rolling(WIN_20, 1).mean()\n",
    "    ma50  = close.rolling(WIN_50, 1).mean()\n",
    "    ma100 = close.rolling(WIN_100,1).mean()\n",
    "    ma200 = close.rolling(WIN_200,1).mean()\n",
    "\n",
    "    # VWAP20\n",
    "    dv = close*vol\n",
    "    vwap20 = (dv.rolling(WIN_20,1).sum())/(vol.rolling(WIN_20,1).sum().replace(0,np.nan))\n",
    "\n",
    "    # volume/dollar-volume signals\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ln_dv = np.where((vol>0)&(close>0), np.log(vol*close), np.nan)\n",
    "    ln_dv = pd.Series(ln_dv, index=g.index)\n",
    "    vol_accel_5  = ln_dv - ln_dv.shift(5)\n",
    "    vol_accel_10 = ln_dv - ln_dv.shift(10)\n",
    "\n",
    "    avg10_dv = dv.rolling(WIN_10,1).mean()\n",
    "    avg60_dv = dv.rolling(WIN_60,1).mean()\n",
    "    std60_dv = dv.rolling(WIN_60,2).std(ddof=1)\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        abn_vol_60d = (avg10_dv - avg60_dv)/std60_dv\n",
    "\n",
    "    # Ann vol\n",
    "    vol20_ann  = pd.Series(lr).rolling(WIN_20,2).std(ddof=1)*np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    vol100_ann = pd.Series(lr).rolling(WIN_100,2).std(ddof=1)*np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    mean100    = pd.Series(lr).rolling(WIN_100,1).mean()\n",
    "\n",
    "    # Range/position\n",
    "    low10  = low.rolling(WIN_10,1).min()\n",
    "    high10 = high.rolling(WIN_10,1).max()\n",
    "    rng10  = high10 - low10\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        pos10 = (close - low10)/rng10\n",
    "    pos10 = pd.Series(pos10, index=g.index).fillna(0.0)\n",
    "    five_day_range_pos = pos10.rolling(SMA_POS_LEN,1).mean()\n",
    "\n",
    "    daily_rng = (high-low)\n",
    "    avg_rng10 = daily_rng.rolling(WIN_10,1).mean()\n",
    "    avg_rng60 = daily_rng.rolling(WIN_60,1).mean()\n",
    "    std_rng60 = daily_rng.rolling(WIN_60,2).std(ddof=1)\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        z_60_10_rng = (avg_rng10 - avg_rng60)/std_rng60\n",
    "    z_60_10_rng = pd.Series(z_60_10_rng, index=g.index).fillna(0.0)\n",
    "\n",
    "    def sret(lag): \n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            return np.where((close>0)&(close.shift(lag)>0), np.log(close/close.shift(lag)), 0.0)\n",
    "    ret5   = sret(5);   ret10  = sret(10); ret20  = sret(20)\n",
    "    ret40  = sret(40);  ret60  = sret(60); ret200 = sret(200); ret300 = sret(300)\n",
    "\n",
    "    med100 = pd.Series(lr).rolling(WIN_100,1).median()\n",
    "\n",
    "    # 100d drawdown percent/duration on calendar dates\n",
    "    closes_by_date = pd.Series(close.values, index=pd.to_datetime(g[\"dt\"]))\n",
    "    def _dd_pct(w: pd.Series) -> float:\n",
    "        w = w.dropna()\n",
    "        if len(w)<=1: return 0.0\n",
    "        dmax = w.idxmax(); maxc = w.loc[dmax]; tail = w.loc[dmax:]\n",
    "        if tail.empty: return 0.0\n",
    "        return float(tail.min()/maxc - 1.0)\n",
    "    def _dd_dur(w: pd.Series) -> float:\n",
    "        w = w.dropna()\n",
    "        if len(w)<=1: return 0.0\n",
    "        dmax = w.idxmax(); dmin = w.loc[dmax:].idxmin()\n",
    "        return float((dmin - dmax).days)\n",
    "    dd_pct_100 = closes_by_date.rolling(WIN_100,1).apply(_dd_pct, raw=False).values\n",
    "    dd_dur_100 = closes_by_date.rolling(WIN_100,1).apply(_dd_dur, raw=False).values\n",
    "\n",
    "    # non-annualized vols\n",
    "    lr_s = pd.Series(lr, index=g.index)\n",
    "    vol5   = lr_s.rolling(WIN_05, WIN_05).std(ddof=1)\n",
    "    vol15  = lr_s.rolling(WIN_15, WIN_15).std(ddof=1)\n",
    "    vol60  = lr_s.rolling(WIN_60, WIN_60).std(ddof=1)\n",
    "    vol252 = lr_s.rolling(WIN_252,WIN_252).std(ddof=1)\n",
    "\n",
    "    neg = np.minimum(lr_s,0.0); pos = np.maximum(lr_s,0.0)\n",
    "    dd15  = (neg.pow(2).rolling(WIN_15,WIN_15).mean())**0.5\n",
    "    dd60  = (neg.pow(2).rolling(WIN_60,WIN_60).mean())**0.5\n",
    "    dd252 = (neg.pow(2).rolling(WIN_252,WIN_252).mean())**0.5\n",
    "    ud15  = (pos.pow(2).rolling(WIN_15,WIN_15).mean())**0.5\n",
    "    ud60  = (pos.pow(2).rolling(WIN_60,WIN_60).mean())**0.5\n",
    "    ud252 = (pos.pow(2).rolling(WIN_252,WIN_252).mean())**0.5\n",
    "\n",
    "    # Parkinson 20d\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        hl_log = np.log((high.replace(0,np.nan)) / (low.replace(0,np.nan)))\n",
    "    k = 1.0/(4.0*math.log(2.0))\n",
    "    pk20 = np.sqrt(k*(hl_log.pow(2).rolling(WIN_20,WIN_20).mean()))\n",
    "\n",
    "    # change in 10d cum log returns\n",
    "    sum10 = lr_s.rolling(WIN_10,WIN_10).sum()\n",
    "    change10 = sum10 - sum10.shift(WIN_10)\n",
    "\n",
    "    # slope accel of log_price over 60d\n",
    "    slope60 = pd.Series(np.log(close.replace(0,np.nan))).rolling(WIN_60,WIN_60).apply(_ols_slope, raw=True)\n",
    "    slope60_prev = slope60.shift(WIN_60)\n",
    "    ret_accel_60 = slope60 - slope60_prev\n",
    "\n",
    "    # vol slopes\n",
    "    slope_vol60_over20  = vol60.rolling(WIN_20,WIN_20).apply(_ols_slope, raw=True)\n",
    "    slope_vol252_over60 = vol252.rolling(WIN_60,WIN_60).apply(_ols_slope, raw=True)\n",
    "\n",
    "    # dollar volume long windows & correlation\n",
    "    dv_sma_252 = dv.rolling(WIN_252,WIN_252).mean()\n",
    "    dv_sma_60  = dv.rolling(WIN_60,WIN_60).mean()\n",
    "    dv252_accel_60 = dv_sma_252.rolling(WIN_60,WIN_60).apply(_ols_slope, raw=True)\n",
    "    corr_px_dv_60  = close.rolling(WIN_60,WIN_60).corr(dv)\n",
    "\n",
    "    ema5_of_vol15 = vol15.ewm(span=5, adjust=False).mean()\n",
    "\n",
    "    mdd_750     = close.rolling(WIN_750,2).apply(_max_dd_only, raw=True)\n",
    "    mdd_dur_750 = close.rolling(WIN_750,2).apply(_max_dd_dur_only, raw=True)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"dt\": g[\"dt\"].values,\n",
    "        \"ticker_id\": g[\"ticker_id\"].values,\n",
    "        \"open\": open_.values,\n",
    "        \"high\": high.values,\n",
    "        \"low\": low.values,\n",
    "        \"adj_close\": close.values,\n",
    "        \"volume\": vol.values,\n",
    "\n",
    "        \"log_returns\": lr_s.values,\n",
    "        \"volatility_20d\": vol20_ann.values,\n",
    "        \"volatility_100d\": vol100_ann.values,\n",
    "        \"mean_return_100d\": mean100.values,\n",
    "        \"moving_avg_20d\": ma20.values,\n",
    "        \"moving_avg_50d\": ma50.values,\n",
    "        \"moving_avg_100d\": ma100.values,\n",
    "        \"moving_avg_200d\": ma200.values,\n",
    "        \"vwap_20d\": vwap20.values,\n",
    "        \"vol_accel_5d\": vol_accel_5.values,\n",
    "        \"vol_accel_10d\": vol_accel_10.values,\n",
    "        \"abn_vol_60d\": abn_vol_60d.values,\n",
    "        \"5_day_range_pos\": five_day_range_pos.values,\n",
    "        \"60_10_highlowrange_zscore\": z_60_10_rng.values,\n",
    "        \"5_day_ret\": ret5,\n",
    "        \"10_day_ret\": ret10,\n",
    "        \"20_day_ret\": ret20,\n",
    "        \"40_day_ret\": ret40,\n",
    "        \"60_day_ret\": ret60,\n",
    "        \"200_day_ret\": ret200,\n",
    "        \"300_day_ret\": ret300,\n",
    "        \"median_return_100d\": med100.values,\n",
    "        \"drawdown_percent\": dd_pct_100,\n",
    "        \"drawdown_duration_days\": dd_dur_100,\n",
    "\n",
    "        \"log_prices\": lp.values,\n",
    "        \"change_10dayret\": change10.values,\n",
    "        \"slope_over60_of_logprice\": slope60.values,\n",
    "        \"prior_slope_over60_of_logprice\": slope60_prev.values,\n",
    "        \"60d_return_accel\": ret_accel_60.values,\n",
    "\n",
    "        \"750d_drawdown\": mdd_750.values,\n",
    "        \"750d_drawdownduration\": mdd_dur_750.values,\n",
    "\n",
    "        \"15d_downsidedeviation\": dd15.values,\n",
    "        \"60d_downsidedeviation\": dd60.values,\n",
    "        \"252d_downsidedeviation\": dd252.values,\n",
    "\n",
    "        \"15d_upsidevolatility\": ud15.values,\n",
    "        \"60d_upsidevolatility\": ud60.values,\n",
    "        \"252d_upsidevolatility\": ud252.values,\n",
    "\n",
    "        \"5d_volatility\": vol5.values,\n",
    "        \"15d_volatility\": vol15.values,\n",
    "        \"60d_volatility\": vol60.values,\n",
    "        \"252d_volatility\": vol252.values,\n",
    "\n",
    "        \"20d_parkinson_HL_volatility\": pk20.values,\n",
    "        \"5d_EMA_15dayvolatility\": ema5_of_vol15.values,\n",
    "\n",
    "        \"slope_over20_of_60d_volatility\": slope_vol60_over20.values,\n",
    "        \"slope_over60_of_252d_volatility\": slope_vol252_over60.values,\n",
    "\n",
    "        \"252d_dollar_volume_SMA\": dv_sma_252.values,\n",
    "        \"60d_dollar_volume_SMA\":  dv_sma_60.values,\n",
    "        \"252d_dollar_volume_accel\": dv252_accel_60.values,\n",
    "        \"60d_price_dollarVolume_correlation\": corr_px_dv_60.values,\n",
    "    })\n",
    "\n",
    "    # clean structural NaNs\n",
    "    out.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
    "    out[[\"5_day_range_pos\",\"60_10_highlowrange_zscore\",\"drawdown_percent\",\"drawdown_duration_days\"]] = \\\n",
    "        out[[\"5_day_range_pos\",\"60_10_highlowrange_zscore\",\"drawdown_percent\",\"drawdown_duration_days\"]].fillna(0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "hist[\"ticker_id\"] = hist[\"ticker\"].map(ticker_map).astype(int)\n",
    "metrics_all = (\n",
    "    pd.concat([compute_metrics(g) for _,g in hist.groupby(\"ticker_id\", sort=False)], ignore_index=True)\n",
    "    if not hist.empty else pd.DataFrame()\n",
    ")\n",
    "\n",
    "# ================== 4) MAP METRIC CODES -> IDs & UPSERT fact_metric_daily ==================\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_metric_daily (\n",
    "  metric_id INTEGER NOT NULL,\n",
    "  dt        DATE    NOT NULL,\n",
    "  ticker_id INTEGER NOT NULL,\n",
    "  value     DOUBLE,\n",
    "  PRIMARY KEY (metric_id, dt, ticker_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "dim_metric = con.execute(\"SELECT metric_id, metric_code FROM dim_metric\").df()\n",
    "metric_map = dict(zip(dim_metric[\"metric_code\"], dim_metric[\"metric_id\"]))\n",
    "\n",
    "PRICE_COLS = {\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}\n",
    "NON_METRIC = {\"ticker_id\",\"dt\"} | PRICE_COLS\n",
    "metric_cols = [c for c in metrics_all.columns if c not in NON_METRIC]\n",
    "\n",
    "# rename metric_code -> metric_id where available\n",
    "rename_dict = {c: metric_map[c] for c in metric_cols if c in metric_map}\n",
    "metrics_for_unpivot = metrics_all.rename(columns=rename_dict).copy()\n",
    "\n",
    "# any columns not present in dim_metric are dropped from write (but kept in memory if needed)\n",
    "kept_cols = [c for c in metrics_for_unpivot.columns if (c in NON_METRIC) or isinstance(c, int)]\n",
    "metrics_for_unpivot = metrics_for_unpivot[kept_cols]\n",
    "\n",
    "# long format\n",
    "long_metrics = metrics_for_unpivot.melt(\n",
    "    id_vars=[\"ticker_id\",\"dt\"],\n",
    "    value_vars=[c for c in metrics_for_unpivot.columns if isinstance(c, int)],\n",
    "    var_name=\"metric_id\",\n",
    "    value_name=\"value\"\n",
    ").dropna(subset=[\"value\"])\n",
    "\n",
    "con.register(\"df_metrics_long\", long_metrics)\n",
    "con.execute(\"\"\"\n",
    "BEGIN TRANSACTION;\n",
    "MERGE INTO fact_metric_daily t\n",
    "USING df_metrics_long s\n",
    "ON t.metric_id = s.metric_id AND t.dt = s.dt AND t.ticker_id = s.ticker_id\n",
    "WHEN MATCHED THEN UPDATE SET value = s.value\n",
    "WHEN NOT MATCHED THEN INSERT (metric_id, dt, ticker_id, value)\n",
    "VALUES (s.metric_id, s.dt, s.ticker_id, s.value);\n",
    "COMMIT;\n",
    "\"\"\")\n",
    "\n",
    "# ================== 5) REFRESH snapshot_metric_latest FOR JUST THESE TICKERS ==================\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS snapshot_metric_latest (\n",
    "  ticker_id INTEGER NOT NULL,\n",
    "  metric_id INTEGER NOT NULL,\n",
    "  dt        DATE    NOT NULL,\n",
    "  value     DOUBLE,\n",
    "  PRIMARY KEY (ticker_id, metric_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# limit refresh to the affected ticker_ids for efficiency\n",
    "id_list_sql = \",\".join(map(str, sorted(set(hist[\"ticker_id\"]))))\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TEMP TABLE _latest_rows AS\n",
    "WITH mx AS (\n",
    "  SELECT ticker_id, metric_id, MAX(dt) AS dt\n",
    "  FROM fact_metric_daily\n",
    "  WHERE ticker_id IN ({id_list_sql})\n",
    "  GROUP BY 1,2\n",
    ")\n",
    "SELECT f.ticker_id, f.metric_id, CAST(f.dt AS DATE) AS dt, f.value\n",
    "FROM fact_metric_daily f\n",
    "JOIN mx\n",
    "  ON f.ticker_id = mx.ticker_id\n",
    " AND f.metric_id = mx.metric_id\n",
    " AND f.dt = mx.dt;\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "BEGIN TRANSACTION;\n",
    "MERGE INTO snapshot_metric_latest t\n",
    "USING _latest_rows s\n",
    "ON t.ticker_id = s.ticker_id AND t.metric_id = s.metric_id\n",
    "WHEN MATCHED AND (t.dt <> s.dt OR (t.value IS DISTINCT FROM s.value)) THEN\n",
    "  UPDATE SET dt = s.dt, value = s.value\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (ticker_id, metric_id, dt, value)\n",
    "  VALUES (s.ticker_id, s.metric_id, s.dt, s.value);\n",
    "COMMIT;\n",
    "\"\"\")\n",
    "\n",
    "# Optional prune (only for these tickers)\n",
    "con.execute(f\"\"\"\n",
    "DELETE FROM snapshot_metric_latest t\n",
    "WHERE t.ticker_id IN ({id_list_sql})\n",
    "  AND NOT EXISTS (\n",
    "    SELECT 1 FROM (\n",
    "      SELECT DISTINCT ticker_id, metric_id FROM fact_metric_daily WHERE ticker_id IN ({id_list_sql})\n",
    "    ) k\n",
    "    WHERE k.ticker_id = t.ticker_id AND k.metric_id = t.metric_id\n",
    "  );\n",
    "\"\"\")\n",
    "\n",
    "# ================== DONE ==================\n",
    "# Quick summary\n",
    "n_new = len(to_insert) if 'to_insert' in locals() else 0\n",
    "print(f\"New tickers inserted into dim_ticker: {n_new}\")\n",
    "print(\"Price rows upserted:\", len(df_prices))\n",
    "print(\"Metric rows upserted:\", len(long_metrics))\n",
    "print(\"Snapshot rows now:\", con.execute(\"SELECT COUNT(*) FROM snapshot_metric_latest\").fetchone()[0])\n",
    "\n",
    "# (Optional) peek a couple of rows for one new ticker\n",
    "sample_id = next(iter(sorted(set(hist[\"ticker_id\"])))) if not hist.empty else None\n",
    "if sample_id is not None:\n",
    "    print(\"\\nSample prices:\", con.execute(f\"\"\"\n",
    "      SELECT * FROM fact_price_daily WHERE ticker_id = {sample_id} ORDER BY dt DESC LIMIT 5\n",
    "    \"\"\").df())\n",
    "    print(\"\\nSample metrics:\", con.execute(f\"\"\"\n",
    "      SELECT * FROM fact_metric_daily WHERE ticker_id = {sample_id} ORDER BY dt DESC LIMIT 5\n",
    "    \"\"\").df())\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d39d5e-94e3-4c21-92ed-fa803333382e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
