{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d75f02a-d1c3-4fd3-b975-41a4974d784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ================== USER SETTINGS ==================\n",
    "API_KEY = \"c5PobUQjaaMTHySILWqmWi9uyIDqYJBi\"\n",
    "\n",
    "SINGLE_DAY_MODE = True        # True -> from=to=TARGET_DATE; False -> use DATE_FROM/DATE_TO\n",
    "TARGET_DATE = \"2025-10-13\"    # YYYY-MM-DD\n",
    "\n",
    "# If SINGLE_DAY_MODE is False, set these:\n",
    "#DATE_FROM = \"2025-09-01\"\n",
    "#DATE_TO   = \"2025-09-25\"\n",
    "# ==================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e764ce-d279-46ca-8cd9-8a786028ba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "Batch 1/6: 100 symbols\n",
      "Batch 2/6: 100 symbols\n",
      "Batch 3/6: 100 symbols\n",
      "Batch 4/6: 100 symbols\n",
      "Batch 5/6: 100 symbols\n",
      "Batch 6/6: 19 symbols\n",
      "\n",
      "Done. Tickers requested: 519\n",
      "Tickers with rows returned: 507\n",
      "Rows fetched: 507\n",
      "\n",
      "No rows returned for the requested date/window:\n",
      "WBA\n",
      "\n",
      "Failed after retries (HTTP/network errors):\n",
      "MET, MGM, MHK, META, MKTX, MLM, MKC, MMC, MMM, MO, MOH\n",
      "\n",
      "Head:\n",
      "         date ticker    open    high     low   close  adjClose    volume\n",
      "0  2025-10-13      A  137.85  139.84  136.76  138.23    138.23   1481500\n",
      "1  2025-10-13   AAPL  249.38  249.69  245.56  247.66    247.66  38142942\n",
      "2  2025-10-13   ABBV  230.00  233.81  229.22  230.30    230.30   5411890\n",
      "3  2025-10-13   ABNB  118.91  119.80  118.25  118.86    118.86   3344832\n",
      "4  2025-10-13    ABT  132.00  132.76  130.87  131.38    131.38   4008306\n"
     ]
    }
   ],
   "source": [
    "db_path = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "tickers_duck = con.execute(\"SELECT ticker FROM dim_ticker\").fetchall()\n",
    "tickers = [t[0] for t in tickers_duck]\n",
    "TICKERS: List[str] = tickers \n",
    "con.close()\n",
    "\n",
    "print(len(tickers))\n",
    "\n",
    "\"\"\"\n",
    "Daily OHLCV downloader (FMP) — concurrent, batched, with retries\n",
    "- Reads from a Python list of tickers (or optional CSV)\n",
    "- Fetches daily bars for a single date (from=to=TARGET_DATE) or a range\n",
    "- Concurrency capped to ~4 in-flight requests (safe for FMP Starter)\n",
    "- Short pauses between batches\n",
    "- Retries 429/5xx with exponential backoff\n",
    "- Prints a summary of tickers with no data for the requested day\n",
    "- Produces a DataFrame `data` (no files written)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ---- Tuning (fast but generally safe for FMP Starter) -------------------------\n",
    "# Parallel workers: FMP commonly allows ~4 parallel requests safely.\n",
    "MAX_WORKERS = 4\n",
    "# Batch size: how many symbols to schedule per wave\n",
    "BATCH_SIZE = 100\n",
    "# Pause between batches (seconds). Keep small to speed up end-to-end.\n",
    "SLEEP_BETWEEN_BATCHES = 2.0\n",
    "# Per-request connect/read timeout\n",
    "REQUEST_TIMEOUT = 20\n",
    "# Max retries for 429/5xx\n",
    "MAX_RETRIES = 3\n",
    "# Backoff base (seconds) for 429/5xx\n",
    "BACKOFF_BASE = 1.5\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _daterange() -> Tuple[str, str]:\n",
    "    if SINGLE_DAY_MODE:\n",
    "        return TARGET_DATE, TARGET_DATE\n",
    "    return DATE_FROM, DATE_TO\n",
    "\n",
    "def fetch_daily(symbol: str, date_from: str, date_to: str) -> Optional[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Fetch daily bars for a symbol over [date_from, date_to].\n",
    "    Retries on 429 and transient 5xx.\n",
    "    Returns list of bar dicts or None on hard failure.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{symbol}\"\n",
    "    params = {\"from\": date_from, \"to\": date_to, \"apikey\": API_KEY}\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        except requests.RequestException:\n",
    "            # transient network error — backoff & retry\n",
    "            if attempt < MAX_RETRIES:\n",
    "                time.sleep(BACKOFF_BASE ** attempt)\n",
    "                continue\n",
    "            return None\n",
    "\n",
    "        # Handle rate limiting / transient server errors\n",
    "        if r.status_code in (429, 502, 503, 504):\n",
    "            if attempt < MAX_RETRIES:\n",
    "                # Try to respect Retry-After if present\n",
    "                retry_after = r.headers.get(\"Retry-After\")\n",
    "                delay = float(retry_after) if retry_after else (BACKOFF_BASE ** attempt)\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            return None\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            # Hard failure; don't retry further\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "        return js.get(\"historical\", [])\n",
    "\n",
    "    return None  # unreachable, but explicit\n",
    "\n",
    "def chunked(lst: List[str], n: int):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "date_from, date_to = _daterange()\n",
    "all_rows: List[Dict] = []\n",
    "no_data: List[str] = []       # symbols that returned zero rows in the requested window\n",
    "hard_fail: List[str] = []     # symbols that errored out after retries\n",
    "\n",
    "total = len(TICKERS)\n",
    "batches = list(chunked(TICKERS, BATCH_SIZE))\n",
    "\n",
    "for bi, batch in enumerate(batches, start=1):\n",
    "    print(f\"Batch {bi}/{len(batches)}: {len(batch)} symbols\")\n",
    "    futures = {}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        for sym in batch:\n",
    "            futures[ex.submit(fetch_daily, sym, date_from, date_to)] = sym\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            sym = futures[fut]\n",
    "            hist = fut.result()\n",
    "            if hist is None:\n",
    "                hard_fail.append(sym)\n",
    "                continue\n",
    "\n",
    "            # Keep only rows that match the exact day in single-day mode\n",
    "            if SINGLE_DAY_MODE:\n",
    "                hist = [h for h in hist if h.get(\"date\") == TARGET_DATE]\n",
    "\n",
    "            if not hist:\n",
    "                no_data.append(sym)\n",
    "                continue\n",
    "\n",
    "            for h in hist:\n",
    "                all_rows.append({\n",
    "                    \"date\": h.get(\"date\"),\n",
    "                    \"ticker\": sym,\n",
    "                    \"open\": h.get(\"open\"),\n",
    "                    \"high\": h.get(\"high\"),\n",
    "                    \"low\": h.get(\"low\"),\n",
    "                    \"close\": h.get(\"close\"),\n",
    "                    \"adjClose\": h.get(\"adjClose\"),\n",
    "                    \"volume\": h.get(\"volume\"),\n",
    "                })\n",
    "\n",
    "    if bi < len(batches):\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "# Build final DataFrame\n",
    "data = pd.DataFrame(all_rows)\n",
    "if not data.empty:\n",
    "    data = data.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "# Progress / diagnostics\n",
    "fetched = data[\"ticker\"].nunique() if not data.empty else 0\n",
    "print(f\"\\nDone. Tickers requested: {total}\")\n",
    "print(f\"Tickers with rows returned: {fetched}\")\n",
    "print(f\"Rows fetched: {len(data)}\")\n",
    "\n",
    "if no_data:\n",
    "    print(\"\\nNo rows returned for the requested date/window:\")\n",
    "    # Show a few, then count\n",
    "    preview = \", \".join(no_data[:20])\n",
    "    more = f\" ... (+{len(no_data)-20} more)\" if len(no_data) > 20 else \"\"\n",
    "    print(preview + more)\n",
    "\n",
    "if hard_fail:\n",
    "    print(\"\\nFailed after retries (HTTP/network errors):\")\n",
    "    preview = \", \".join(hard_fail[:20])\n",
    "    more = f\" ... (+{len(hard_fail)-20} more)\" if len(hard_fail) > 20 else \"\"\n",
    "    print(preview + more)\n",
    "\n",
    "# Show head for quick inspection\n",
    "print(\"\\nHead:\")\n",
    "print(data.head())\n",
    "\n",
    "# XLB, ZTS, XLC, XLE, XLI, XLK, XLP, XLF, XLV, XLRE, XLU, XLY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac6790b-b7bb-47ce-8547-8909b28fa276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ticker ticker_type         dt    open    high     low  close  \\\n",
      "387614    ZTS       Stock 2025-10-06  146.43  147.04  144.85    NaN   \n",
      "387615    ZTS       Stock 2025-10-07  146.20  146.20  142.15    NaN   \n",
      "387616    ZTS       Stock 2025-10-08  142.46  144.87  142.28    NaN   \n",
      "387617    ZTS       Stock 2025-10-09  143.82  145.22  143.25    NaN   \n",
      "387618    ZTS       Stock 2025-10-10  143.39  143.49  140.83    NaN   \n",
      "\n",
      "        adj_close   volume  \n",
      "387614     145.36  3114876  \n",
      "387615     142.77  2745916  \n",
      "387616     143.49  3016455  \n",
      "387617     143.39  3505222  \n",
      "387618     141.11  2958395  \n",
      "  ticker ticker_type                   dt    open    high     low   close  \\\n",
      "0      A       Stock  2022-10-14 00:00:00  129.00  130.22  125.47  125.70   \n",
      "1      A       Stock  2022-10-17 00:00:00  127.38  131.09  127.38  130.56   \n",
      "2      A       Stock  2022-10-18 00:00:00  133.92  134.68  131.20  132.30   \n",
      "3      A       Stock  2022-10-19 00:00:00  130.11  130.27  127.24  128.96   \n",
      "4      A       Stock  2022-10-20 00:00:00  127.83  129.74  125.61  125.94   \n",
      "\n",
      "   adj_close   volume  \n",
      "0     123.23  1217213  \n",
      "1     127.99  1197928  \n",
      "2     129.70  1038147  \n",
      "3     126.42  1034000  \n",
      "4     123.46  1897350   \n",
      "        ticker ticker_type                   dt    open    high     low  \\\n",
      "388121    ZTS       Stock  2025-10-07 00:00:00  146.20  146.20  142.15   \n",
      "388122    ZTS       Stock  2025-10-08 00:00:00  142.46  144.87  142.28   \n",
      "388123    ZTS       Stock  2025-10-09 00:00:00  143.82  145.22  143.25   \n",
      "388124    ZTS       Stock  2025-10-10 00:00:00  143.39  143.49  140.83   \n",
      "388125    ZTS       Stock           2025-10-13  141.26  143.36  140.65   \n",
      "\n",
      "         close  adj_close   volume  \n",
      "388121     NaN     142.77  2745916  \n",
      "388122     NaN     143.49  3016455  \n",
      "388123     NaN     143.39  3505222  \n",
      "388124     NaN     141.11  2958395  \n",
      "388125  142.26     142.26  2261300  \n",
      "Daily metrics rows for 2025-10-13: 507\n",
      "507\n",
      "[INFO] 5 columns did not match any metric_code -> metric_id mapping (kept as-is):\n",
      "['open', 'high', 'low', 'adj_close', 'volume'] \n",
      "        date ticker_type    open    high     low  adj_close      volume  \\\n",
      "0 2025-10-13       Stock  137.85  139.84  136.76     138.23   1481500.0   \n",
      "1 2025-10-13       Stock  249.38  249.69  245.56     247.66  38142942.0   \n",
      "2 2025-10-13       Stock  230.00  233.81  229.22     230.30   5411890.0   \n",
      "3 2025-10-13       Stock  118.91  119.80  118.25     118.86   3344832.0   \n",
      "4 2025-10-13       Stock  132.00  132.76  130.87     131.38   4008306.0   \n",
      "\n",
      "         36        52        51  ...        13         8        18        46  \\\n",
      "0  0.011716  0.360190  0.309132  ...  0.020202  0.017370  0.025083  0.000116   \n",
      "1  0.009697  0.255667  0.235281  ...  0.020215  0.012349  0.014510  0.000055   \n",
      "2 -0.000868  0.296928  0.214733  ...  0.018421  0.014126  0.020881  0.000158   \n",
      "3  0.005653  0.173277  0.266848  ...  0.024271  0.012852  0.009854 -0.000048   \n",
      "4 -0.009017  0.125258  0.216498  ...  0.013751  0.012258  0.007573 -0.000183   \n",
      "\n",
      "         47             9            22            10        24  ticker_id  \n",
      "0  0.000019  2.351634e+08  2.059952e+08 -2.233017e+04  0.468136          1  \n",
      "1  0.000009  1.195334e+10  1.284643e+10  5.673343e+06  0.022450          2  \n",
      "2  0.000007  1.216366e+09  1.106699e+09  8.122295e+05  0.526680          3  \n",
      "3 -0.000033  6.580850e+08  6.112746e+08  1.006489e+05 -0.226783          4  \n",
      "4 -0.000002  7.871437e+08  7.362349e+08  5.363381e+05 -0.172240          5  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "Inserted/updated prices for rows: 507\n",
      "Inserted/updated metric rows: 24753\n",
      "daily_metrics columns after dropping price fields: ['dt', 'ticker_type', 36, 52, 51, 38, 42, 43, 40, 41, 53, 50, 49, 31, 16, 20, 17, 1, 7, 15, 21, 6, 14, 39, 34, 33, 35, 32, 48, 44, 25, 28, 29, 3, 23, 11, 4, 26, 12, 19, 5, 27, 13, 8, 18, 46, 47, 9, 22, 10, 24, 'ticker_id']\n",
      "Rows in snapshot: 26445\n",
      "   ticker_id  metric_id         dt         value\n",
      "0          1          1 2025-10-13  1.106556e-01\n",
      "1          1          2 2025-09-19  1.618045e+05\n",
      "2          1          3 2025-10-13  1.067426e-02\n",
      "3          1          4 2025-10-13  2.339297e-02\n",
      "4          1          5 2025-10-13  2.588064e-02\n",
      "5          1          6 2025-10-13  3.264756e-02\n",
      "6          1          7 2025-10-13  9.557789e-02\n",
      "7          1          8 2025-10-13  1.736991e-02\n",
      "8          1          9 2025-10-13  2.351634e+08\n",
      "9          1         10 2025-10-13 -2.233017e+04\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "# READING NECESSARY HISTORICAL INFORMATION TO DO ANALYSIS CALCS\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "DB_PATH = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "ROWS_PER_TICKER = 750  # most recent N rows per ticker\n",
    "\n",
    "# Connect read-only (avoids lock conflicts)\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "# Pull prices + ticker_type so our analytics can toggle volume usage\n",
    "sql = f\"\"\"\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    f.ticker_id,\n",
    "    t.ticker,\n",
    "    t.ticker_type,\n",
    "    f.dt,\n",
    "    f.open, f.high, f.low, f.close, f.adj_close, f.volume,\n",
    "    ROW_NUMBER() OVER (PARTITION BY f.ticker_id ORDER BY f.dt DESC) AS rn\n",
    "  FROM fact_price_daily AS f\n",
    "  JOIN dim_ticker       AS t USING (ticker_id)\n",
    ")\n",
    "SELECT\n",
    "  ticker, ticker_type, dt, open, high, low, close, adj_close, volume\n",
    "FROM ranked\n",
    "WHERE rn <= {ROWS_PER_TICKER}\n",
    "ORDER BY ticker, dt\n",
    "\"\"\"\n",
    "\n",
    "prices = con.execute(sql).df()\n",
    "print(\"\\n\", prices.tail())\n",
    "\n",
    "# COMBINES TODAY'S PRICE DATA TO HISTORICAL DATABASE STOCK PRICE DATA\n",
    "\n",
    "# Step 1: Align column names in 'data'\n",
    "data_renamed = data.rename(columns={\n",
    "    \"date\": \"dt\",\n",
    "    \"adjClose\": \"adj_close\"\n",
    "})\n",
    "\n",
    "# Attach ticker_type to today's data rows using dim_ticker (so analytics knows whether to use volume)\n",
    "dim_t = con.execute(\"SELECT ticker, ticker_type FROM dim_ticker\").df()\n",
    "ticker_to_type = dict(zip(dim_t[\"ticker\"], dim_t[\"ticker_type\"]))\n",
    "data_renamed[\"ticker_type\"] = data_renamed[\"ticker\"].map(ticker_to_type)\n",
    "\n",
    "# Step 2: Ensure same column order as 'prices' (ticker_type comes along as well)\n",
    "# prices has: ticker, ticker_type, dt, open, high, low, close, adj_close, volume\n",
    "data_renamed = data_renamed[[\"ticker\", \"ticker_type\", \"dt\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]]\n",
    "\n",
    "# Step 3: Combine with prices\n",
    "combined = pd.concat([prices, data_renamed], ignore_index=True)\n",
    "\n",
    "# Step 4: Sort by ticker, then date\n",
    "combined = combined.sort_values([\"ticker\", \"dt\"]).reset_index(drop=True)\n",
    "\n",
    "# (Optional) quick check\n",
    "print(combined.head(), \"\\n\", combined.tail())\n",
    "\n",
    "\n",
    "# ANALYSIS ON OHLCV DATA. SPICY STUFF.\n",
    "\n",
    "# Expect TARGET_DATE to already be defined by you; this just normalizes it\n",
    "TARGET_DATE = pd.to_datetime(TARGET_DATE)\n",
    "\n",
    "# ----------------------- CONFIG -----------------------\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "# Windows (trading-day counts)\n",
    "WIN_05   = 5\n",
    "WIN_10   = 10\n",
    "WIN_15   = 15\n",
    "WIN_20   = 20\n",
    "WIN_50   = 50\n",
    "WIN_60   = 60\n",
    "WIN_100  = 100\n",
    "WIN_200  = 200\n",
    "WIN_252  = 252\n",
    "WIN_300  = 300\n",
    "WIN_750  = 750\n",
    "SMA_POS_LEN = 3\n",
    "OBV_BASELINE_DATE = pd.Timestamp(\"2004-01-01\")  # kept for reproducibility, not used now\n",
    "\n",
    "# -------------------- Helper functions --------------------\n",
    "def ols_slope_window(arr: np.ndarray) -> float:\n",
    "    mask = np.isfinite(arr)\n",
    "    y = arr[mask]\n",
    "    n = y.size\n",
    "    if n < 2:\n",
    "        return np.nan\n",
    "    x = np.arange(n, dtype=float)\n",
    "    xm, ym = x.mean(), y.mean()\n",
    "    denom = np.sum((x - xm) ** 2)\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    num = np.sum((x - xm) * (y - ym))\n",
    "    return float(num / denom)\n",
    "\n",
    "def max_drawdown_only(arr: np.ndarray) -> float:\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    if not np.isfinite(a).any():\n",
    "        return np.nan\n",
    "    start_idx = -1\n",
    "    peak = np.nan\n",
    "    for i in range(a.size):\n",
    "        if math.isfinite(a[i]) and a[i] > 0.0:\n",
    "            peak = a[i]\n",
    "            start_idx = i\n",
    "            break\n",
    "    if start_idx < 0:\n",
    "        return np.nan\n",
    "    best_dd = 0.0\n",
    "    for j in range(start_idx + 1, a.size):\n",
    "        pj = a[j]\n",
    "        if not math.isfinite(pj) or pj <= 0.0:\n",
    "            continue\n",
    "        if pj > peak:\n",
    "            peak = pj\n",
    "        dd = pj / peak - 1.0\n",
    "        if dd < best_dd:\n",
    "            best_dd = dd\n",
    "    return float(best_dd)\n",
    "\n",
    "def max_drawdown_duration_only(arr: np.ndarray) -> float:\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    if not np.isfinite(a).any():\n",
    "        return np.nan\n",
    "    max_price = np.nan\n",
    "    max_idx = -1\n",
    "    for i in range(a.size):\n",
    "        if math.isfinite(a[i]) and a[i] > 0.0:\n",
    "            max_price = a[i]\n",
    "            max_idx = i\n",
    "            break\n",
    "    if max_idx < 0:\n",
    "        return np.nan\n",
    "    best_dd = 0.0\n",
    "    best_dur = 0\n",
    "    for j in range(max_idx + 1, a.size):\n",
    "        pj = a[j]\n",
    "        if not math.isfinite(pj) or pj <= 0.0:\n",
    "            continue\n",
    "        if pj > max_price:\n",
    "            max_price = pj\n",
    "            max_idx = j\n",
    "        dd = pj / max_price - 1.0\n",
    "        if dd < best_dd:\n",
    "            best_dd = dd\n",
    "            best_dur = j - max_idx\n",
    "    return float(best_dur)\n",
    "\n",
    "def per_ticker_metrics_all_rows(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute metrics for a single ticker (sorted by date).\n",
    "\n",
    "    Fixes:\n",
    "    - De-dupes by calendar date to avoid duplicate index labels.\n",
    "    - Drawdown helpers use position-based (NumPy) indexing to return scalars even when\n",
    "      labels repeat; robust to NaNs.\n",
    "\n",
    "    Behavior:\n",
    "    - If g.ticker_type != 'Stock' (case-insensitive), volume-based metrics are NaN.\n",
    "    - Price-only metrics computed for all ticker types.\n",
    "    \"\"\"\n",
    "    # ---- Normalize, sort, and de-duplicate by date (CRITICAL FIX) -------------\n",
    "    g = g.copy()\n",
    "    if \"date\" not in g.columns and \"dt\" in g.columns:\n",
    "        g[\"date\"] = pd.to_datetime(g[\"dt\"])\n",
    "    else:\n",
    "        g[\"date\"] = pd.to_datetime(g[\"date\"])\n",
    "    g = g.sort_values(\"date\").drop_duplicates(subset=\"date\", keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    # Determine whether to use volume\n",
    "    ttype = (str(g[\"ticker_type\"].iloc[0]) if \"ticker_type\" in g.columns else \"Stock\").strip().lower()\n",
    "    uses_volume = (ttype == \"stock\")\n",
    "\n",
    "    # Base series\n",
    "    close = pd.to_numeric(g[\"adj_close\"], errors=\"coerce\").astype(float)\n",
    "    open_ = pd.to_numeric(g[\"open\"],      errors=\"coerce\").astype(float)\n",
    "    high  = pd.to_numeric(g[\"high\"],      errors=\"coerce\").astype(float)\n",
    "    low   = pd.to_numeric(g[\"low\"],       errors=\"coerce\").astype(float)\n",
    "\n",
    "    # Volume series for storage and for calculations (NaN if not Stock)\n",
    "    if \"volume\" in g.columns:\n",
    "        vol_original = pd.to_numeric(g[\"volume\"], errors=\"coerce\").astype(float)\n",
    "    else:\n",
    "        vol_original = pd.Series(np.nan, index=g.index, dtype=float)\n",
    "    vol_calc = vol_original if uses_volume else pd.Series(np.nan, index=g.index, dtype=float)\n",
    "\n",
    "    # Safe variants for logs/divisions\n",
    "    close_safe = close.replace(0, np.nan)\n",
    "\n",
    "    # ---------- Core logs & returns ----------\n",
    "    log_price = np.log(close_safe)\n",
    "    prev_close = close_safe.shift(1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        log_returns = np.where((close_safe > 0) & (prev_close > 0),\n",
    "                               np.log(close_safe / prev_close), np.nan)\n",
    "    log_returns = pd.Series(log_returns, index=g.index)\n",
    "    if len(g) > 0 and pd.isna(log_returns.iloc[0]):\n",
    "        log_returns.iloc[0] = 0.0\n",
    "\n",
    "    # ---------- Moving averages / VWAP ----------\n",
    "    WIN_05, WIN_10, WIN_15, WIN_20, WIN_50 = 5, 10, 15, 20, 50\n",
    "    WIN_60, WIN_100, WIN_200, WIN_252, WIN_300, WIN_750 = 60, 100, 200, 252, 300, 750\n",
    "    TRADING_DAYS_PER_YEAR = 252\n",
    "    SMA_POS_LEN = 3\n",
    "\n",
    "    ma20   = close.rolling(WIN_20,  min_periods=1).mean()\n",
    "    ma50   = close.rolling(WIN_50,  min_periods=1).mean()\n",
    "    ma100  = close.rolling(WIN_100, min_periods=1).mean()\n",
    "    ma200  = close.rolling(WIN_200, min_periods=1).mean()\n",
    "\n",
    "    # All dollar-volume things use vol_calc (becomes NaN for non-Stock)\n",
    "    dv = close * vol_calc\n",
    "\n",
    "    # vwap20 (NaN if not using volume)\n",
    "    sum_px_vol_20 = dv.rolling(WIN_20, min_periods=1).sum()\n",
    "    sum_vol_20    = vol_calc.rolling(WIN_20, min_periods=1).sum()\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        vwap20 = np.where(sum_vol_20 > 0, sum_px_vol_20 / sum_vol_20, np.nan)\n",
    "\n",
    "    # Dollar-volume accelerations\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ln_dv = np.where((vol_calc > 0) & (close > 0), np.log(vol_calc * close), np.nan)\n",
    "    ln_dv = pd.Series(ln_dv, index=g.index)\n",
    "    vol_accel_5d  = ln_dv - ln_dv.shift(5)\n",
    "    vol_accel_10d = ln_dv - ln_dv.shift(10)\n",
    "\n",
    "    # 10v60 abnormal volume\n",
    "    avg10_dv = dv.rolling(WIN_10, min_periods=1).mean()\n",
    "    avg60_dv = dv.rolling(WIN_60, min_periods=1).mean()\n",
    "    std60_dv = dv.rolling(WIN_60, min_periods=2).std(ddof=1)\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        abn_vol_60d = np.where(std60_dv > 0, (avg10_dv - avg60_dv) / std60_dv, np.nan)\n",
    "\n",
    "    # Volatility (annualized) & mean return 100d\n",
    "    vol20_ann  = log_returns.rolling(WIN_20,  min_periods=2).std(ddof=1) * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    vol100_ann = log_returns.rolling(WIN_100, min_periods=2).std(ddof=1) * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    mean100    = log_returns.rolling(WIN_100, min_periods=1).mean()\n",
    "\n",
    "    # Range/position block\n",
    "    low_10  = low.rolling(WIN_10, min_periods=1).min()\n",
    "    high_10 = high.rolling(WIN_10, min_periods=1).max()\n",
    "    rng_10  = high_10 - low_10\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        pos_10d = np.where(rng_10 != 0, (close - low_10) / rng_10, 0.0)\n",
    "    pos_10d = pd.Series(pos_10d, index=g.index)\n",
    "    five_day_range_pos = pos_10d.rolling(SMA_POS_LEN, min_periods=1).mean()\n",
    "\n",
    "    daily_range = (high - low)\n",
    "    avg_rng_10  = daily_range.rolling(WIN_10, min_periods=1).mean()\n",
    "    avg_rng_60  = daily_range.rolling(WIN_60, min_periods=1).mean()\n",
    "    std_rng_60  = daily_range.rolling(WIN_60, min_periods=2).std(ddof=1)\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        z_60_10_highlowrange = np.where(std_rng_60 > 0, (avg_rng_10 - avg_rng_60) / std_rng_60, 0.0)\n",
    "\n",
    "    # Multi-horizon log returns (use 0.0 when lag missing)\n",
    "    def safe_lr(curr, lagged):\n",
    "        return np.where((curr > 0) & (lagged > 0), np.log(curr / lagged), 0.0)\n",
    "\n",
    "    ret5   = safe_lr(close, close.shift(5))\n",
    "    ret10  = safe_lr(close, close.shift(10))\n",
    "    ret20  = safe_lr(close, close.shift(20))\n",
    "    ret40  = safe_lr(close, close.shift(40))\n",
    "    ret60  = safe_lr(close, close.shift(60))\n",
    "    ret200 = safe_lr(close, close.shift(200))\n",
    "    ret300 = safe_lr(close, close.shift(300))\n",
    "\n",
    "    # Median return over last 100 trading days\n",
    "    median_return_100d = log_returns.rolling(WIN_100, min_periods=1).median()\n",
    "\n",
    "    # ---------- Robust rolling drawdown (position-based) ----------\n",
    "    closes_by_date = pd.Series(close.values, index=g[\"date\"])\n",
    "\n",
    "    def _dd_percent(w: pd.Series) -> float:\n",
    "        v = pd.to_numeric(w, errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
    "        if v.size <= 1 or not np.isfinite(v).any():\n",
    "            return 0.0\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            try:\n",
    "                peak_pos = np.nanargmax(v)\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "            peak = v[peak_pos]\n",
    "            if not np.isfinite(peak):\n",
    "                return 0.0\n",
    "            suffix = v[peak_pos:]\n",
    "            if suffix.size == 0 or not np.isfinite(suffix).any():\n",
    "                return 0.0\n",
    "            trough = np.nanmin(suffix)\n",
    "            return float(trough / peak - 1.0)\n",
    "\n",
    "    def _dd_duration(w: pd.Series) -> float:\n",
    "        v = pd.to_numeric(w, errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
    "        idx = w.index\n",
    "        if v.size <= 1 or not np.isfinite(v).any():\n",
    "            return 0.0\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            try:\n",
    "                peak_pos = np.nanargmax(v)\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "            suffix = v[peak_pos:]\n",
    "            if suffix.size == 0 or not np.isfinite(suffix).any():\n",
    "                return 0.0\n",
    "            trough_rel = np.nanargmin(suffix)\n",
    "            trough_pos = peak_pos + trough_rel\n",
    "            # calendar-day duration\n",
    "            return float((idx[trough_pos] - idx[peak_pos]).days)\n",
    "\n",
    "    drawdown_percent_100 = closes_by_date.rolling(WIN_100, min_periods=1).apply(_dd_percent, raw=False).values\n",
    "    drawdown_days_100    = closes_by_date.rolling(WIN_100, min_periods=1).apply(_dd_duration, raw=False).values\n",
    "\n",
    "    # ---------- Extended analytics ----------\n",
    "    lr = pd.Series(log_returns, index=g.index)\n",
    "    vol_5   = lr.rolling(WIN_05,  min_periods=WIN_05).std(ddof=1)\n",
    "    vol_15  = lr.rolling(WIN_15,  min_periods=WIN_15).std(ddof=1)\n",
    "    vol_60  = lr.rolling(WIN_60,  min_periods=WIN_60).std(ddof=1)\n",
    "    vol_252 = lr.rolling(WIN_252, min_periods=WIN_252).std(ddof=1)\n",
    "\n",
    "    neg = np.minimum(lr, 0.0)\n",
    "    pos = np.maximum(lr, 0.0)\n",
    "    dd_15  = (pd.Series(neg).pow(2).rolling(WIN_15,  min_periods=WIN_15).mean()) ** 0.5\n",
    "    dd_60  = (pd.Series(neg).pow(2).rolling(WIN_60,  min_periods=WIN_60).mean()) ** 0.5\n",
    "    dd_252 = (pd.Series(neg).pow(2).rolling(WIN_252, min_periods=WIN_252).mean()) ** 0.5\n",
    "    ud_15  = (pd.Series(pos).pow(2).rolling(WIN_15,  min_periods=WIN_15).mean()) ** 0.5\n",
    "    ud_60  = (pd.Series(pos).pow(2).rolling(WIN_60,  min_periods=WIN_60).mean()) ** 0.5\n",
    "    ud_252 = (pd.Series(pos).pow(2).rolling(WIN_252, min_periods=WIN_252).mean()) ** 0.5\n",
    "\n",
    "    # Parkinson HL volatility (20d), non-annualized\n",
    "    hl_log = np.log((high.replace(0, np.nan)) / (low.replace(0, np.nan)))\n",
    "    k = 1.0 / (4.0 * math.log(2.0))\n",
    "    pk20 = np.sqrt(k * (hl_log.pow(2).rolling(WIN_20, min_periods=WIN_20).mean()))\n",
    "\n",
    "    # Change in 10-day cumulative log returns\n",
    "    sum10 = lr.rolling(WIN_10, min_periods=WIN_10).sum()\n",
    "    change_10dayret = sum10 - sum10.shift(WIN_10)\n",
    "\n",
    "    # 60d return acceleration via slopes of log_price\n",
    "    def ols_slope_window(arr: np.ndarray) -> float:\n",
    "        mask = np.isfinite(arr)\n",
    "        y = arr[mask]\n",
    "        n = y.size\n",
    "        if n < 2:\n",
    "            return np.nan\n",
    "        x = np.arange(n, dtype=float)\n",
    "        xm, ym = x.mean(), y.mean()\n",
    "        denom = np.sum((x - xm) ** 2)\n",
    "        if denom == 0:\n",
    "            return np.nan\n",
    "        num = np.sum((x - xm) * (y - ym))\n",
    "        return float(num / denom)\n",
    "\n",
    "    slope_lp_recent60 = pd.Series(np.log(close.replace(0, np.nan))).rolling(WIN_60, min_periods=WIN_60).apply(ols_slope_window, raw=True)\n",
    "    slope_lp_prev60   = slope_lp_recent60.shift(WIN_60)\n",
    "    ret_accel_60      = slope_lp_recent60 - slope_lp_prev60\n",
    "\n",
    "    # Vol slopes\n",
    "    slope_vol60_over20  = vol_60.rolling(WIN_20, min_periods=WIN_20).apply(ols_slope_window, raw=True)\n",
    "    slope_vol252_over60 = vol_252.rolling(WIN_60, min_periods=WIN_60).apply(ols_slope_window, raw=True)\n",
    "\n",
    "    # Dollar volume long windows & correlation (NaN for non-Stock)\n",
    "    dv_sma_252 = dv.rolling(WIN_252, min_periods=WIN_252).mean()\n",
    "    dv_sma_60  = dv.rolling(WIN_60,  min_periods=WIN_60).mean()\n",
    "    dv252_accel_60 = dv_sma_252.rolling(WIN_60, min_periods=WIN_60).apply(ols_slope_window, raw=True)\n",
    "    corr_px_dv_60  = close.rolling(WIN_60, min_periods=WIN_60).corr(dv)\n",
    "\n",
    "    # EMA(5) of 15d volatility\n",
    "    ema5_of_vol15 = vol_15.ewm(span=5, adjust=False).mean()\n",
    "\n",
    "    # Rolling 750-day max drawdown & duration (in trading days)\n",
    "    def max_drawdown_only(arr: np.ndarray) -> float:\n",
    "        a = np.asarray(arr, dtype=float)\n",
    "        if not np.isfinite(a).any():\n",
    "            return np.nan\n",
    "        start_idx = -1\n",
    "        peak = np.nan\n",
    "        for i in range(a.size):\n",
    "            if math.isfinite(a[i]) and a[i] > 0.0:\n",
    "                peak = a[i]\n",
    "                start_idx = i\n",
    "                break\n",
    "        if start_idx < 0:\n",
    "            return np.nan\n",
    "        best_dd = 0.0\n",
    "        for j in range(start_idx + 1, a.size):\n",
    "            pj = a[j]\n",
    "            if not math.isfinite(pj) or pj <= 0.0:\n",
    "                continue\n",
    "            if pj > peak:\n",
    "                peak = pj\n",
    "            dd = pj / peak - 1.0\n",
    "            if dd < best_dd:\n",
    "                best_dd = dd\n",
    "        return float(best_dd)\n",
    "\n",
    "    def max_drawdown_duration_only(arr: np.ndarray) -> float:\n",
    "        a = np.asarray(arr, dtype=float)\n",
    "        if not np.isfinite(a).any():\n",
    "            return np.nan\n",
    "        max_price = np.nan\n",
    "        max_idx = -1\n",
    "        for i in range(a.size):\n",
    "            if math.isfinite(a[i]) and a[i] > 0.0:\n",
    "                max_price = a[i]\n",
    "                max_idx = i\n",
    "                break\n",
    "        if max_idx < 0:\n",
    "            return np.nan\n",
    "        best_dd = 0.0\n",
    "        best_dur = 0\n",
    "        for j in range(max_idx + 1, a.size):\n",
    "            pj = a[j]\n",
    "            if not math.isfinite(pj) or pj <= 0.0:\n",
    "                continue\n",
    "            if pj > max_price:\n",
    "                max_price = pj\n",
    "                max_idx = j\n",
    "            dd = pj / max_price - 1.0\n",
    "            if dd < best_dd:\n",
    "                best_dd = dd\n",
    "                best_dur = j - max_idx\n",
    "        return float(best_dur)\n",
    "\n",
    "    mdd_750     = close.rolling(WIN_750, min_periods=2).apply(max_drawdown_only, raw=True)\n",
    "    mdd_dur_750 = close.rolling(WIN_750, min_periods=2).apply(max_drawdown_duration_only, raw=True)\n",
    "\n",
    "    # ---------- Assemble ----------\n",
    "    out = pd.DataFrame({\n",
    "        \"date\": g[\"date\"].values,\n",
    "        \"ticker\": g[\"ticker\"].values,\n",
    "        \"ticker_type\": g.get(\"ticker_type\", pd.Series([\"Stock\"]*len(g), index=g.index)).values,\n",
    "\n",
    "        # Original OHLCV for price upsert (volume preserved even if not used in calcs)\n",
    "        \"open\": open_.values,\n",
    "        \"high\": high.values,\n",
    "        \"low\": low.values,\n",
    "        \"adj_close\": close.values,\n",
    "        \"volume\": vol_original.values,\n",
    "\n",
    "        # SQL-parity / price-volume set (momentum labels removed)\n",
    "        \"log_returns\": log_returns.values,\n",
    "        \"volatility_20d\": vol20_ann.values,\n",
    "        \"volatility_100d\": vol100_ann.values,\n",
    "        \"mean_return_100d\": mean100.values,\n",
    "        \"moving_avg_20d\": ma20.values,\n",
    "        \"moving_avg_50d\": ma50.values,\n",
    "        \"moving_avg_100d\": ma100.values,\n",
    "        \"moving_avg_200d\": ma200.values,\n",
    "        \"vwap_20d\": vwap20,                         # NaN for non-Stock\n",
    "        \"vol_accel_5d\": vol_accel_5d.values,        # NaN for non-Stock\n",
    "        \"vol_accel_10d\": vol_accel_10d.values,      # NaN for non-Stock\n",
    "        \"abn_vol_60d\": abn_vol_60d,                 # NaN for non-Stock\n",
    "        \"5_day_range_pos\": five_day_range_pos.values,\n",
    "        \"60_10_highlowrange_zscore\": z_60_10_highlowrange,\n",
    "        \"5_day_ret\": ret5,\n",
    "        \"10_day_ret\": ret10,\n",
    "        \"20_day_ret\": ret20,\n",
    "        \"40_day_ret\": ret40,\n",
    "        \"60_day_ret\": ret60,\n",
    "        \"200_day_ret\": ret200,\n",
    "        \"300_day_ret\": ret300,\n",
    "        \"median_return_100d\": median_return_100d.values,\n",
    "        \"drawdown_percent\": drawdown_percent_100,\n",
    "        \"drawdown_duration_days\": drawdown_days_100,\n",
    "\n",
    "        # Extended analytics (no OBV fields)\n",
    "        \"log_prices\": log_price.values,\n",
    "        \"change_10dayret\": change_10dayret.values,\n",
    "        \"slope_over60_of_logprice\": slope_lp_recent60.values,\n",
    "        \"prior_slope_over60_of_logprice\": slope_lp_prev60.values,\n",
    "        \"60d_return_accel\": ret_accel_60.values,\n",
    "\n",
    "        \"750d_drawdown\": mdd_750.values,\n",
    "        \"750d_drawdownduration\": mdd_dur_750.values,\n",
    "\n",
    "        \"15d_downsidedeviation\": dd_15.values,\n",
    "        \"60d_downsidedeviation\": dd_60.values,\n",
    "        \"252d_downsidedeviation\": dd_252.values,\n",
    "\n",
    "        \"15d_upsidevolatility\": ud_15.values,\n",
    "        \"60d_upsidevolatility\": ud_60.values,\n",
    "        \"252d_upsidevolatility\": ud_252.values,\n",
    "\n",
    "        \"5d_volatility\": vol_5.values,\n",
    "        \"15d_volatility\": vol_15.values,\n",
    "        \"60d_volatility\": vol_60.values,\n",
    "        \"252d_volatility\": vol_252.values,\n",
    "\n",
    "        \"20d_parkinson_HL_volatility\": pk20.values,\n",
    "        \"5d_EMA_15dayvolatility\": ema5_of_vol15.values,\n",
    "\n",
    "        \"slope_over20_of_60d_volatility\": slope_vol60_over20.values,\n",
    "        \"slope_over60_of_252d_volatility\": slope_vol252_over60.values,\n",
    "\n",
    "        # Dollar-volume family (all NaN for non-Stock)\n",
    "        \"252d_dollar_volume_SMA\": dv_sma_252.values,\n",
    "        \"60d_dollar_volume_SMA\":  dv_sma_60.values,\n",
    "        \"252d_dollar_volume_accel\": dv252_accel_60.values,\n",
    "        \"60d_price_dollarVolume_correlation\": corr_px_dv_60.values,\n",
    "    }, index=g.index)\n",
    "\n",
    "    # Clean edge cases\n",
    "    out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Zero-fill only structural indicators where needed for SQL parity\n",
    "    out[[\n",
    "        \"5_day_range_pos\",\"60_10_highlowrange_zscore\",\n",
    "        \"drawdown_percent\",\"drawdown_duration_days\"\n",
    "    ]] = out[[\n",
    "        \"5_day_range_pos\",\"60_10_highlowrange_zscore\",\n",
    "        \"drawdown_percent\",\"drawdown_duration_days\"\n",
    "    ]].fillna(0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------- Run for TARGET_DATE (from `combined`) --------------------\n",
    "_ = combined.copy()\n",
    "_.columns = [c.strip().lower() for c in _.columns]\n",
    "\n",
    "# Build a proper datetime column named 'date'\n",
    "if \"dt\" in _.columns and \"date\" not in _.columns:\n",
    "    _[\"date\"] = pd.to_datetime(_[\"dt\"])\n",
    "else:\n",
    "    _[\"date\"] = pd.to_datetime(_[\"date\"])\n",
    "\n",
    "for c in [\"open\",\"high\",\"low\",\"close\",\"adj_close\"]:\n",
    "    _[c] = pd.to_numeric(_[c], errors=\"coerce\")\n",
    "if \"volume\" in _.columns:\n",
    "    _[\"volume\"] = pd.to_numeric(_[\"volume\"], errors=\"coerce\")\n",
    "\n",
    "# Ensure ticker_type exists (fallback Stock if missing)\n",
    "if \"ticker_type\" not in _.columns:\n",
    "    dim_t = con.execute(\"SELECT ticker, ticker_type FROM dim_ticker\").df()\n",
    "    ticker_to_type = dict(zip(dim_t[\"ticker\"], dim_t[\"ticker_type\"]))\n",
    "    _[\"ticker_type\"] = _[\"ticker\"].map(ticker_to_type).fillna(\"Stock\")\n",
    "\n",
    "_ = _.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Compare on PURE DATE to avoid dtype/timezone issues\n",
    "T_DATE = pd.Timestamp(TARGET_DATE).date()\n",
    "\n",
    "daily_chunks = []\n",
    "for tkr, g in _.groupby(\"ticker\", sort=False):\n",
    "    # If this ticker has any row on the calendar date, keep it\n",
    "    if not (g[\"date\"].dt.date == T_DATE).any():\n",
    "        continue\n",
    "    all_rows = per_ticker_metrics_all_rows(g)\n",
    "    day_row = all_rows[all_rows[\"date\"].dt.date == T_DATE]\n",
    "    if not day_row.empty:\n",
    "        daily_chunks.append(day_row)\n",
    "\n",
    "daily_metrics = (\n",
    "    pd.concat(daily_chunks, ignore_index=True)\n",
    "      .sort_values([\"ticker\",\"date\"])\n",
    "      .reset_index(drop=True)\n",
    "    if daily_chunks else pd.DataFrame()\n",
    ")\n",
    "\n",
    "print(f\"Daily metrics rows for {T_DATE}: {len(daily_metrics):,}\")\n",
    "daily_metrics.head()\n",
    "\n",
    "num_rows = len(data)\n",
    "print(num_rows)\n",
    "\n",
    "# CHANGING DATA TO DATABASE FORMAT\n",
    "\n",
    "# You already have daily_metrics as a pandas DataFrame\n",
    "# Assumes it has at least: 'ticker' column, plus metric columns named by metric_code, and possibly 'date' etc.\n",
    "\n",
    "db_path = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "# --- Connect & load dimensions ---\n",
    "con.close()\n",
    "con = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "dim_ticker = con.execute(\"\"\"\n",
    "    SELECT ticker_id, ticker\n",
    "    FROM dim_ticker\n",
    "\"\"\").df()\n",
    "\n",
    "dim_metric = con.execute(\"\"\"\n",
    "    SELECT metric_id, metric_code\n",
    "    FROM dim_metric\n",
    "\"\"\").df()\n",
    "\n",
    "con.close()\n",
    "\n",
    "# --- Map ticker -> ticker_id ---\n",
    "ticker_map = dict(zip(dim_ticker[\"ticker\"], dim_ticker[\"ticker_id\"]))\n",
    "daily_metrics[\"ticker_id\"] = daily_metrics[\"ticker\"].map(ticker_map)\n",
    "\n",
    "# Optional: sanity check for unmapped tickers\n",
    "unmapped_tickers = daily_metrics.loc[daily_metrics[\"ticker_id\"].isna(), \"ticker\"].unique()\n",
    "if len(unmapped_tickers):\n",
    "    print(f\"[WARN] {len(unmapped_tickers)} tickers not found in dim_ticker:\", unmapped_tickers[:10], \"...\" if len(unmapped_tickers) > 10 else \"\")\n",
    "\n",
    "# Drop original ticker (keep ticker_type only if you want; it’s not written to fact tables)\n",
    "daily_metrics = daily_metrics.drop(columns=[\"ticker\"])\n",
    "\n",
    "# --- Rename metric_code columns -> metric_id ---\n",
    "metric_map = dict(zip(dim_metric[\"metric_code\"], dim_metric[\"metric_id\"]))\n",
    "\n",
    "# Build rename dict only for columns that match known metric_codes\n",
    "rename_dict = {col: metric_map[col] for col in daily_metrics.columns if col in metric_map}\n",
    "\n",
    "# Apply rename\n",
    "daily_metrics = daily_metrics.rename(columns=rename_dict)\n",
    "\n",
    "# Optional: report which metric columns could not be mapped\n",
    "metric_like_cols = [c for c in daily_metrics.columns if c not in {\"date\", \"ticker_id\", \"ticker_type\"}]\n",
    "unmapped_metrics = [c for c in metric_like_cols if c not in metric_map.values()]  # after rename, mapped ones are metric_ids\n",
    "if unmapped_metrics:\n",
    "    print(f\"[INFO] {len(unmapped_metrics)} columns did not match any metric_code -> metric_id mapping (kept as-is):\")\n",
    "    print(unmapped_metrics[:20], \"...\" if len(unmapped_metrics) > 20 else \"\")\n",
    "\n",
    "# --- Show result ---\n",
    "print(daily_metrics.head())\n",
    "\n",
    "# SENDING THE DATA TO THE DUCK DATABASE\n",
    "\n",
    "# --- Existing connection (adjust if needed) ---\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# --- Assumes you already have a pandas DataFrame named `daily_metrics` in memory ---\n",
    "PRICE_COLS_ORDERED = [\"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
    "\n",
    "# 1) Normalize date column -> 'dt' (date type)\n",
    "df = daily_metrics.copy()\n",
    "if \"dt\" not in df.columns and \"date\" in df.columns:\n",
    "    df = df.rename(columns={\"date\": \"dt\"})\n",
    "if \"dt\" not in df.columns:\n",
    "    raise ValueError(\"daily_metrics must have a 'dt' or 'date' column.\")\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"]).dt.date\n",
    "\n",
    "# Keep ticker_type for debugging if present (not written to fact tables)\n",
    "if \"ticker_type\" in df.columns:\n",
    "    df_price_debug = df[[\"ticker_id\",\"dt\",\"ticker_type\"] + [c for c in PRICE_COLS_ORDERED if c in df.columns]].copy()\n",
    "\n",
    "# 2) Insert/Upsert into fact_price_daily\n",
    "present_price_cols = [c for c in PRICE_COLS_ORDERED if c in df.columns]\n",
    "missing_id_cols = [c for c in [\"ticker_id\"] if c not in df.columns]\n",
    "if missing_id_cols:\n",
    "    raise ValueError(f\"Missing required ID columns in daily_metrics: {missing_id_cols}\")\n",
    "\n",
    "if present_price_cols:\n",
    "    price_insert_cols = [\"ticker_id\", \"dt\"] + present_price_cols\n",
    "    df_price = df[price_insert_cols].copy()\n",
    "    con.register(\"df_price\", df_price)\n",
    "\n",
    "    set_clauses = \", \".join([f\"{c} = s.{c}\" for c in present_price_cols])  # no qualifier on left\n",
    "    insert_cols = \", \".join(price_insert_cols)\n",
    "    insert_vals = \", \".join([f\"s.{c}\" for c in price_insert_cols])\n",
    "\n",
    "    merge_sql_price = f\"\"\"\n",
    "    BEGIN TRANSACTION;\n",
    "\n",
    "    MERGE INTO fact_price_daily t\n",
    "    USING df_price s\n",
    "    ON t.ticker_id = s.ticker_id AND t.dt = s.dt\n",
    "    WHEN MATCHED THEN UPDATE SET {set_clauses}\n",
    "    WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals});\n",
    "\n",
    "    COMMIT;\n",
    "    \"\"\"\n",
    "    con.execute(merge_sql_price)\n",
    "else:\n",
    "    df_price = pd.DataFrame(columns=[\"ticker_id\",\"dt\"])  # for counts later\n",
    "\n",
    "# 3) Drop price columns from the working DataFrame\n",
    "df_no_price = df.drop(columns=[c for c in PRICE_COLS_ORDERED if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# 4) Unpivot metrics and upsert into fact_metric_daily\n",
    "NON_METRIC = set([\"ticker_id\", \"dt\", \"ticker_type\"] + PRICE_COLS_ORDERED)\n",
    "metric_cols = [c for c in df_no_price.columns if c not in NON_METRIC]\n",
    "\n",
    "if metric_cols:\n",
    "    long_metrics = df_no_price.melt(\n",
    "        id_vars=[\"ticker_id\", \"dt\"],\n",
    "        value_vars=metric_cols,\n",
    "        var_name=\"metric_id\",\n",
    "        value_name=\"value\"\n",
    "    ).dropna(subset=[\"value\"])\n",
    "\n",
    "    con.register(\"df_metrics_long\", long_metrics)\n",
    "\n",
    "    merge_sql_metrics = \"\"\"\n",
    "    BEGIN TRANSACTION;\n",
    "\n",
    "    MERGE INTO fact_metric_daily t\n",
    "    USING df_metrics_long s\n",
    "    ON t.ticker_id = s.ticker_id AND t.dt = s.dt AND t.metric_id = s.metric_id\n",
    "    WHEN MATCHED THEN UPDATE SET value = s.value\n",
    "    WHEN NOT MATCHED THEN INSERT (metric_id, dt, ticker_id, value)\n",
    "    VALUES (s.metric_id, s.dt, s.ticker_id, s.value);\n",
    "\n",
    "    COMMIT;\n",
    "    \"\"\"\n",
    "    con.execute(merge_sql_metrics)\n",
    "else:\n",
    "    long_metrics = pd.DataFrame(columns=[\"ticker_id\",\"dt\",\"metric_id\",\"value\"])\n",
    "\n",
    "# 5) Update your in-session daily_metrics without price fields\n",
    "daily_metrics = df_no_price\n",
    "\n",
    "# Quick peek\n",
    "print(\"Inserted/updated prices for rows:\", len(df_price))\n",
    "print(\"Inserted/updated metric rows:\", len(long_metrics))\n",
    "print(\"daily_metrics columns after dropping price fields:\", list(daily_metrics.columns))\n",
    "\n",
    "# 1) Ensure snapshot table exists (idempotent)\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS snapshot_metric_latest (\n",
    "  ticker_id INTEGER NOT NULL,\n",
    "  metric_id INTEGER NOT NULL,\n",
    "  dt        DATE    NOT NULL,\n",
    "  value     DOUBLE,\n",
    "  PRIMARY KEY (ticker_id, metric_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 2) Materialize latest rows WITHOUT window functions (no QUALIFY)\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TEMP TABLE _latest_rows AS\n",
    "WITH mx AS (\n",
    "  SELECT ticker_id, metric_id, MAX(dt) AS dt\n",
    "  FROM fact_metric_daily\n",
    "  GROUP BY 1,2\n",
    ")\n",
    "SELECT\n",
    "  f.ticker_id,\n",
    "  f.metric_id,\n",
    "  CAST(f.dt AS DATE) AS dt,\n",
    "  f.value\n",
    "FROM fact_metric_daily f\n",
    "JOIN mx\n",
    "  ON f.ticker_id = mx.ticker_id\n",
    " AND f.metric_id = mx.metric_id\n",
    " AND f.dt = mx.dt;\n",
    "\"\"\")\n",
    "\n",
    "# 3) Upsert into snapshot via MERGE\n",
    "con.execute(\"\"\"\n",
    "MERGE INTO snapshot_metric_latest AS t\n",
    "USING _latest_rows AS s\n",
    "ON t.ticker_id = s.ticker_id AND t.metric_id = s.metric_id\n",
    "WHEN MATCHED AND (t.dt <> s.dt OR (t.value IS DISTINCT FROM s.value)) THEN\n",
    "  UPDATE SET dt = s.dt, value = s.value\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (ticker_id, metric_id, dt, value)\n",
    "  VALUES (s.ticker_id, s.metric_id, s.dt, s.value);\n",
    "\"\"\")\n",
    "\n",
    "# 4) Optional: prune keys that no longer exist in fact_metric_daily\n",
    "con.execute(\"\"\"\n",
    "DELETE FROM snapshot_metric_latest t\n",
    "WHERE NOT EXISTS (\n",
    "  SELECT 1 FROM (\n",
    "    SELECT DISTINCT ticker_id, metric_id FROM fact_metric_daily\n",
    "  ) k\n",
    "  WHERE k.ticker_id = t.ticker_id AND k.metric_id = t.metric_id\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 5) Sanity check\n",
    "print(\"Rows in snapshot:\", con.execute(\"SELECT COUNT(*) FROM snapshot_metric_latest\").fetchone()[0])\n",
    "print(con.execute(\"\"\"\n",
    "  SELECT * FROM snapshot_metric_latest\n",
    "  ORDER BY ticker_id, metric_id\n",
    "  LIMIT 10\n",
    "\"\"\").fetchdf())\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e3642-aaa8-451d-8b17-63fe6ed81ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d2201c-5e62-4334-8bdb-ecf7204783b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ticker         dt    open    high     low  close  adj_close   volume\n",
      "376081    ZTS 2025-09-26  141.86  143.79  141.27    NaN     143.50  2524344\n",
      "376082    ZTS 2025-09-29  143.86  144.15  142.50    NaN     143.06  2854165\n",
      "376083    ZTS 2025-09-30  142.89  147.00  142.40    NaN     146.32  3699138\n",
      "376084    ZTS 2025-10-01  146.32  147.38  145.00    NaN     146.95  3669613\n",
      "376085    ZTS 2025-10-06  146.43  147.04  144.85    NaN     145.36  3114876\n",
      "  ticker                   dt    open    high     low   close  adj_close  \\\n",
      "0      A  2022-10-10 00:00:00  128.05  128.05  124.31  125.95     123.47   \n",
      "1      A  2022-10-11 00:00:00  125.00  127.89  124.16  125.64     123.17   \n",
      "2      A  2022-10-12 00:00:00  126.20  127.10  125.27  125.69     123.22   \n",
      "3      A  2022-10-13 00:00:00  123.00  128.83  122.35  127.90     125.38   \n",
      "4      A  2022-10-14 00:00:00  129.00  130.22  125.47  125.70     123.23   \n",
      "\n",
      "    volume  \n",
      "0  1246005  \n",
      "1  1985348  \n",
      "2  1069633  \n",
      "3  1553600  \n",
      "4  1217213   \n",
      "        ticker                   dt    open    high     low   close  adj_close  \\\n",
      "376549    ZTS  2025-09-29 00:00:00  143.86  144.15  142.50     NaN     143.06   \n",
      "376550    ZTS  2025-09-30 00:00:00  142.89  147.00  142.40     NaN     146.32   \n",
      "376551    ZTS  2025-10-01 00:00:00  146.32  147.38  145.00     NaN     146.95   \n",
      "376552    ZTS  2025-10-06 00:00:00  146.43  147.04  144.85     NaN     145.36   \n",
      "376553    ZTS           2025-10-07  146.20  146.20  142.15  142.77     142.77   \n",
      "\n",
      "         volume  \n",
      "376549  2854165  \n",
      "376550  3699138  \n",
      "376551  3669613  \n",
      "376552  3114876  \n",
      "376553  2745916  \n",
      "Daily metrics rows for 2025-10-07: 468\n",
      "468\n",
      "[INFO] 5 columns did not match any metric_code -> metric_id mapping (kept as-is):\n",
      "['open', 'high', 'low', 'adj_close', 'volume'] \n",
      "        date    open    high     low  adj_close      volume        36  \\\n",
      "0 2025-10-07  141.86  142.27  138.44     138.56   1361834.0 -0.021773   \n",
      "1 2025-10-07  256.81  257.40  255.43     256.48  31643194.0 -0.000818   \n",
      "2 2025-10-07  231.29  234.08  227.59     232.83   4804358.0  0.011404   \n",
      "3 2025-10-07  120.49  121.10  119.14     119.85   4408800.0 -0.004163   \n",
      "4 2025-10-07  134.09  134.09  131.93     133.02   4786505.0 -0.005398   \n",
      "\n",
      "         52        51        38  ...        13         8        18        46  \\\n",
      "0  0.375291  0.305304  0.002192  ...  0.020107  0.017165  0.023199  0.000163   \n",
      "1  0.250088  0.227565  0.001900  ...  0.020165  0.011309  0.015105  0.000058   \n",
      "2  0.324501  0.217723  0.002803  ...  0.018429  0.014403  0.020114  0.000117   \n",
      "3  0.173554  0.271249 -0.001370  ...  0.024422  0.012404  0.011939 -0.000039   \n",
      "4  0.176391  0.223599  0.000385  ...  0.013751  0.013638  0.008862 -0.000056   \n",
      "\n",
      "         47             9            22            10        24  ticker_id  \n",
      "0  0.000016  2.336214e+08  2.089971e+08 -5.409373e+04  0.415005          1  \n",
      "1  0.000009  1.190683e+10  1.277170e+10  5.687718e+06  0.104510          2  \n",
      "2  0.000003  1.211407e+09  1.100488e+09  6.100340e+05  0.512872          3  \n",
      "3 -0.000035  6.571412e+08  6.090908e+08  3.915590e+04 -0.314978          4  \n",
      "4 -0.000001  7.859570e+08  8.227458e+08  4.755324e+05 -0.529379          5  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "Inserted/updated prices for rows: 468\n",
      "Inserted/updated metric rows: 22932\n",
      "daily_metrics columns after dropping price fields: ['dt', 36, 52, 51, 38, 42, 43, 40, 41, 53, 50, 49, 31, 16, 20, 17, 1, 7, 15, 21, 6, 14, 39, 34, 33, 35, 32, 48, 44, 25, 28, 29, 3, 23, 11, 4, 26, 12, 19, 5, 27, 13, 8, 18, 46, 47, 9, 22, 10, 24, 'ticker_id']\n",
      "Rows in snapshot: 25653\n",
      "   ticker_id  metric_id         dt         value\n",
      "0          1          1 2025-10-07  9.549784e-02\n",
      "1          1          2 2025-09-19  1.618045e+05\n",
      "2          1          3 2025-10-07  8.818627e-03\n",
      "3          1          4 2025-10-07  2.289372e-02\n",
      "4          1          5 2025-10-07  2.469763e-02\n",
      "5          1          6 2025-10-07  2.137285e-02\n",
      "6          1          7 2025-10-07  9.303938e-02\n",
      "7          1          8 2025-10-07  1.716525e-02\n",
      "8          1          9 2025-10-07  2.336214e+08\n",
      "9          1         10 2025-10-07 -5.409373e+04\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "#READING NECESSARY HISTORICAL INFORMATION TO DO ANALYSIS CALCS\n",
    "\n",
    "DB_PATH = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "ROWS_PER_TICKER = 750  # most recent N rows per ticker\n",
    "\n",
    "# Connect read-only (avoids lock conflicts)\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "sql = f\"\"\"\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    f.ticker_id,\n",
    "    t.ticker,\n",
    "    f.dt,\n",
    "    f.open, f.high, f.low, f.close, f.adj_close, f.volume,\n",
    "    ROW_NUMBER() OVER (PARTITION BY f.ticker_id ORDER BY f.dt DESC) AS rn\n",
    "  FROM fact_price_daily AS f\n",
    "  JOIN dim_ticker       AS t USING (ticker_id)\n",
    ")\n",
    "SELECT\n",
    "  ticker, dt, open, high, low, close, adj_close, volume\n",
    "FROM ranked\n",
    "WHERE rn <= {ROWS_PER_TICKER}\n",
    "ORDER BY ticker, dt\n",
    "\"\"\"\n",
    "\n",
    "prices = con.execute(sql).df()\n",
    "print( \"\\n\", prices.tail())\n",
    "\n",
    "# COMBINES TODAYS PRICE DATA TO HISTORICAL DATABASE STOCK PRICE DATA\n",
    "\n",
    "# Step 1: Align column names in 'data'\n",
    "data_renamed = data.rename(columns={\n",
    "    \"date\": \"dt\",\n",
    "    \"adjClose\": \"adj_close\"\n",
    "})\n",
    "\n",
    "# Step 2: Ensure same column order as 'prices'\n",
    "data_renamed = data_renamed[[\"ticker\", \"dt\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]]\n",
    "\n",
    "# Step 3: Combine with prices\n",
    "combined = pd.concat([prices, data_renamed], ignore_index=True)\n",
    "\n",
    "# Step 4: Sort by ticker, then date\n",
    "combined = combined.sort_values([\"ticker\", \"dt\"]).reset_index(drop=True)\n",
    "\n",
    "# (Optional) quick check\n",
    "print(combined.head(), \"\\n\", combined.tail())\n",
    "\n",
    "\n",
    "# ANALYSIS ON OHLCV DATA. SPICY STUFF.\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Expect TARGET_DATE to already be defined by you; this just normalizes it\n",
    "TARGET_DATE = pd.to_datetime(TARGET_DATE)\n",
    "\n",
    "# ----------------------- CONFIG -----------------------\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "# Windows (trading-day counts)\n",
    "WIN_05   = 5\n",
    "WIN_10   = 10\n",
    "WIN_15   = 15\n",
    "WIN_20   = 20\n",
    "WIN_50   = 50\n",
    "WIN_60   = 60\n",
    "WIN_100  = 100\n",
    "WIN_200  = 200\n",
    "WIN_252  = 252\n",
    "WIN_300  = 300\n",
    "WIN_750  = 750\n",
    "SMA_POS_LEN = 3\n",
    "OBV_BASELINE_DATE = pd.Timestamp(\"2004-01-01\")  # kept for reproducibility, not used now\n",
    "\n",
    "# -------------------- Helper functions --------------------\n",
    "def ols_slope_window(arr: np.ndarray) -> float:\n",
    "    mask = np.isfinite(arr)\n",
    "    y = arr[mask]\n",
    "    n = y.size\n",
    "    if n < 2:\n",
    "        return np.nan\n",
    "    x = np.arange(n, dtype=float)\n",
    "    xm, ym = x.mean(), y.mean()\n",
    "    denom = np.sum((x - xm) ** 2)\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    num = np.sum((x - xm) * (y - ym))\n",
    "    return float(num / denom)\n",
    "\n",
    "def max_drawdown_only(arr: np.ndarray) -> float:\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    if not np.isfinite(a).any():\n",
    "        return np.nan\n",
    "    start_idx = -1\n",
    "    peak = np.nan\n",
    "    for i in range(a.size):\n",
    "        if math.isfinite(a[i]) and a[i] > 0.0:\n",
    "            peak = a[i]\n",
    "            start_idx = i\n",
    "            break\n",
    "    if start_idx < 0:\n",
    "        return np.nan\n",
    "    best_dd = 0.0\n",
    "    for j in range(start_idx + 1, a.size):\n",
    "        pj = a[j]\n",
    "        if not math.isfinite(pj) or pj <= 0.0:\n",
    "            continue\n",
    "        if pj > peak:\n",
    "            peak = pj\n",
    "        dd = pj / peak - 1.0\n",
    "        if dd < best_dd:\n",
    "            best_dd = dd\n",
    "    return float(best_dd)\n",
    "\n",
    "def max_drawdown_duration_only(arr: np.ndarray) -> float:\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    if not np.isfinite(a).any():\n",
    "        return np.nan\n",
    "    max_price = np.nan\n",
    "    max_idx = -1\n",
    "    for i in range(a.size):\n",
    "        if math.isfinite(a[i]) and a[i] > 0.0:\n",
    "            max_price = a[i]\n",
    "            max_idx = i\n",
    "            break\n",
    "    if max_idx < 0:\n",
    "        return np.nan\n",
    "    best_dd = 0.0\n",
    "    best_dur = 0\n",
    "    for j in range(max_idx + 1, a.size):\n",
    "        pj = a[j]\n",
    "        if not math.isfinite(pj) or pj <= 0.0:\n",
    "            continue\n",
    "        if pj > max_price:\n",
    "            max_price = pj\n",
    "            max_idx = j\n",
    "        dd = pj / max_price - 1.0\n",
    "        if dd < best_dd:\n",
    "            best_dd = dd\n",
    "            best_dur = j - max_idx\n",
    "    return float(best_dur)\n",
    "\n",
    "def per_ticker_metrics_all_rows(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute metrics for a single ticker (sorted by date).\"\"\"\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "\n",
    "    # Base series\n",
    "    close = g[\"adj_close\"].astype(float)\n",
    "    open_ = g[\"open\"].astype(float)\n",
    "    high  = g[\"high\"].astype(float)\n",
    "    low   = g[\"low\"].astype(float)\n",
    "    if \"volume\" in g.columns:\n",
    "        vol = pd.to_numeric(g[\"volume\"], errors=\"coerce\").astype(float)\n",
    "    else:\n",
    "        vol = pd.Series(np.nan, index=g.index, dtype=float)\n",
    "\n",
    "    # Safe variants for logs/divisions\n",
    "    close_safe = close.replace(0, np.nan)\n",
    "\n",
    "    # ---------- Core logs & returns ----------\n",
    "    log_price = np.log(close_safe)\n",
    "    prev_close = close_safe.shift(1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        log_returns = np.where((close_safe > 0) & (prev_close > 0),\n",
    "                               np.log(close_safe / prev_close), np.nan)\n",
    "    log_returns = pd.Series(log_returns, index=g.index)\n",
    "    if len(g) > 0 and pd.isna(log_returns.iloc[0]):\n",
    "        log_returns.iloc[0] = 0.0\n",
    "\n",
    "    # ---------- Moving averages / VWAP (momentum labels removed) ----------\n",
    "    ma20   = close.rolling(WIN_20,  min_periods=1).mean()\n",
    "    ma50   = close.rolling(WIN_50,  min_periods=1).mean()\n",
    "    ma100  = close.rolling(WIN_100, min_periods=1).mean()\n",
    "    ma200  = close.rolling(WIN_200, min_periods=1).mean()\n",
    "\n",
    "    dv = close * vol\n",
    "    sum_px_vol_20 = dv.rolling(WIN_20, min_periods=1).sum()\n",
    "    sum_vol_20    = vol.rolling(WIN_20, min_periods=1).sum()\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        vwap20 = np.where(sum_vol_20 > 0, sum_px_vol_20 / sum_vol_20, np.nan)\n",
    "\n",
    "    # Dollar-volume accelerations\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ln_dv = np.where((vol > 0) & (close > 0), np.log(vol * close), np.nan)\n",
    "    ln_dv = pd.Series(ln_dv, index=g.index)\n",
    "    vol_accel_5d  = ln_dv - ln_dv.shift(5)\n",
    "    vol_accel_10d = ln_dv - ln_dv.shift(10)\n",
    "\n",
    "    # 10v60 abnormal volume\n",
    "    avg10_dv = dv.rolling(WIN_10, min_periods=1).mean()\n",
    "    avg60_dv = dv.rolling(WIN_60, min_periods=1).mean()\n",
    "    std60_dv = dv.rolling(WIN_60, min_periods=2).std(ddof=1)\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        abn_vol_60d = np.where(std60_dv > 0, (avg10_dv - avg60_dv) / std60_dv, np.nan)\n",
    "\n",
    "    # Volatility (annualized) & mean return 100d\n",
    "    vol20_ann  = log_returns.rolling(WIN_20,  min_periods=2).std(ddof=1) * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    vol100_ann = log_returns.rolling(WIN_100, min_periods=2).std(ddof=1) * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    mean100    = log_returns.rolling(WIN_100, min_periods=1).mean()\n",
    "\n",
    "    # Range/position block\n",
    "    low_10  = low.rolling(WIN_10, min_periods=1).min()\n",
    "    high_10 = high.rolling(WIN_10, min_periods=1).max()\n",
    "    rng_10  = high_10 - low_10\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        pos_10d = np.where(rng_10 != 0, (close - low_10) / rng_10, 0.0)\n",
    "    pos_10d = pd.Series(pos_10d, index=g.index)\n",
    "    five_day_range_pos = pos_10d.rolling(SMA_POS_LEN, min_periods=1).mean()\n",
    "\n",
    "    daily_range = (high - low)\n",
    "    avg_rng_10  = daily_range.rolling(WIN_10, min_periods=1).mean()\n",
    "    avg_rng_60  = daily_range.rolling(WIN_60, min_periods=1).mean()\n",
    "    std_rng_60  = daily_range.rolling(WIN_60, min_periods=2).std(ddof=1)\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        z_60_10_highlowrange = np.where(std_rng_60 > 0, (avg_rng_10 - avg_rng_60) / std_rng_60, 0.0)\n",
    "\n",
    "    # Multi-horizon log returns (use 0.0 when lag missing)\n",
    "    def safe_lr(curr, lagged):\n",
    "        return np.where((curr > 0) & (lagged > 0), np.log(curr / lagged), 0.0)\n",
    "\n",
    "    ret5   = safe_lr(close, close.shift(5))\n",
    "    ret10  = safe_lr(close, close.shift(10))\n",
    "    ret20  = safe_lr(close, close.shift(20))\n",
    "    ret40  = safe_lr(close, close.shift(40))\n",
    "    ret60  = safe_lr(close, close.shift(60))\n",
    "    ret200 = safe_lr(close, close.shift(200))\n",
    "    ret300 = safe_lr(close, close.shift(300))\n",
    "\n",
    "    # Median return over last 100 trading days\n",
    "    median_return_100d = log_returns.rolling(WIN_100, min_periods=1).median()\n",
    "\n",
    "    # 100-day rolling drawdown percent & duration (calendar-day duration)\n",
    "    closes_by_date = pd.Series(close.values, index=g[\"date\"])\n",
    "    def _dd_percent(window: pd.Series) -> float:\n",
    "        w = window.dropna()\n",
    "        if len(w) <= 1:\n",
    "            return 0.0\n",
    "        dmax = w.idxmax()\n",
    "        max_close = w.loc[dmax]\n",
    "        suffix = w.loc[dmax:]\n",
    "        if suffix.empty:\n",
    "            return 0.0\n",
    "        return float(suffix.min() / max_close - 1.0)\n",
    "\n",
    "    def _dd_duration(window: pd.Series) -> float:\n",
    "        w = window.dropna()\n",
    "        if len(w) <= 1:\n",
    "            return 0.0\n",
    "        dmax = w.idxmax()\n",
    "        dmin = w.loc[dmax:].idxmin()\n",
    "        return float((dmin - dmax).days)\n",
    "\n",
    "    drawdown_percent_100 = closes_by_date.rolling(WIN_100, min_periods=1).apply(_dd_percent, raw=False).values\n",
    "    drawdown_days_100    = closes_by_date.rolling(WIN_100, min_periods=1).apply(_dd_duration, raw=False).values\n",
    "\n",
    "    # ---------- Extended analytics (non-annualized vols, deviations, slopes, etc.) ----------\n",
    "    lr = pd.Series(log_returns, index=g.index)\n",
    "    vol_5   = lr.rolling(WIN_05,  min_periods=WIN_05).std(ddof=1)\n",
    "    vol_15  = lr.rolling(WIN_15,  min_periods=WIN_15).std(ddof=1)\n",
    "    vol_60  = lr.rolling(WIN_60,  min_periods=WIN_60).std(ddof=1)\n",
    "    vol_252 = lr.rolling(WIN_252, min_periods=WIN_252).std(ddof=1)\n",
    "\n",
    "    neg = np.minimum(lr, 0.0)\n",
    "    pos = np.maximum(lr, 0.0)\n",
    "    dd_15  = (pd.Series(neg).pow(2).rolling(WIN_15,  min_periods=WIN_15).mean()) ** 0.5\n",
    "    dd_60  = (pd.Series(neg).pow(2).rolling(WIN_60,  min_periods=WIN_60).mean()) ** 0.5\n",
    "    dd_252 = (pd.Series(neg).pow(2).rolling(WIN_252, min_periods=WIN_252).mean()) ** 0.5\n",
    "    ud_15  = (pd.Series(pos).pow(2).rolling(WIN_15,  min_periods=WIN_15).mean()) ** 0.5\n",
    "    ud_60  = (pd.Series(pos).pow(2).rolling(WIN_60,  min_periods=WIN_60).mean()) ** 0.5\n",
    "    ud_252 = (pd.Series(pos).pow(2).rolling(WIN_252, min_periods=WIN_252).mean()) ** 0.5\n",
    "\n",
    "    # Parkinson HL volatility (20d), non-annualized\n",
    "    hl_log = np.log((high.replace(0, np.nan)) / (low.replace(0, np.nan)))\n",
    "    k = 1.0 / (4.0 * math.log(2.0))\n",
    "    pk20 = np.sqrt(k * (hl_log.pow(2).rolling(WIN_20, min_periods=WIN_20).mean()))\n",
    "\n",
    "    # Change in 10-day cumulative log returns\n",
    "    sum10 = lr.rolling(WIN_10, min_periods=WIN_10).sum()\n",
    "    change_10dayret = sum10 - sum10.shift(WIN_10)\n",
    "\n",
    "    # 60d return acceleration via slopes of log_price\n",
    "    slope_lp_recent60 = pd.Series(np.log(close.replace(0, np.nan))).rolling(WIN_60, min_periods=WIN_60).apply(ols_slope_window, raw=True)\n",
    "    slope_lp_prev60   = slope_lp_recent60.shift(WIN_60)\n",
    "    ret_accel_60      = slope_lp_recent60 - slope_lp_prev60\n",
    "\n",
    "    # Vol slopes\n",
    "    slope_vol60_over20  = vol_60.rolling(WIN_20, min_periods=WIN_20).apply(ols_slope_window, raw=True)\n",
    "    slope_vol252_over60 = vol_252.rolling(WIN_60, min_periods=WIN_60).apply(ols_slope_window, raw=True)\n",
    "\n",
    "    # Dollar volume long windows & correlation\n",
    "    dv_sma_252 = dv.rolling(WIN_252, min_periods=WIN_252).mean()\n",
    "    dv_sma_60  = dv.rolling(WIN_60,  min_periods=WIN_60).mean()\n",
    "    dv252_accel_60 = dv_sma_252.rolling(WIN_60, min_periods=WIN_60).apply(ols_slope_window, raw=True)\n",
    "    corr_px_dv_60  = close.rolling(WIN_60, min_periods=WIN_60).corr(dv)\n",
    "\n",
    "    # EMA(5) of 15d volatility\n",
    "    ema5_of_vol15 = vol_15.ewm(span=5, adjust=False).mean()\n",
    "\n",
    "    # Rolling 750-day max drawdown & duration (in trading days)\n",
    "    mdd_750     = close.rolling(WIN_750, min_periods=2).apply(max_drawdown_only, raw=True)\n",
    "    mdd_dur_750 = close.rolling(WIN_750, min_periods=2).apply(max_drawdown_duration_only, raw=True)\n",
    "\n",
    "    # ---------- Assemble ----------\n",
    "    out = pd.DataFrame({\n",
    "        \"date\": g[\"date\"].values,\n",
    "        \"ticker\": g[\"ticker\"].values,\n",
    "\n",
    "        # Original OHLCV (with \"adj_close\" as the working close)\n",
    "        \"open\": open_.values,\n",
    "        \"high\": high.values,\n",
    "        \"low\": low.values,\n",
    "        \"adj_close\": close.values,\n",
    "        \"volume\": vol.values,\n",
    "\n",
    "        # SQL-parity / price-volume set (momentum labels removed)\n",
    "        \"log_returns\": lr.values,\n",
    "        \"volatility_20d\": vol20_ann.values,\n",
    "        \"volatility_100d\": vol100_ann.values,\n",
    "        \"mean_return_100d\": mean100.values,\n",
    "        \"moving_avg_20d\": ma20.values,\n",
    "        \"moving_avg_50d\": ma50.values,\n",
    "        \"moving_avg_100d\": ma100.values,\n",
    "        \"moving_avg_200d\": ma200.values,\n",
    "        \"vwap_20d\": vwap20,\n",
    "        \"vol_accel_5d\": vol_accel_5d.values,\n",
    "        \"vol_accel_10d\": vol_accel_10d.values,\n",
    "        \"abn_vol_60d\": abn_vol_60d,\n",
    "        \"5_day_range_pos\": five_day_range_pos.values,\n",
    "        \"60_10_highlowrange_zscore\": z_60_10_highlowrange,\n",
    "        \"5_day_ret\": ret5,\n",
    "        \"10_day_ret\": ret10,\n",
    "        \"20_day_ret\": ret20,\n",
    "        \"40_day_ret\": ret40,\n",
    "        \"60_day_ret\": ret60,\n",
    "        \"200_day_ret\": ret200,\n",
    "        \"300_day_ret\": ret300,\n",
    "        \"median_return_100d\": median_return_100d.values,\n",
    "        \"drawdown_percent\": drawdown_percent_100,\n",
    "        \"drawdown_duration_days\": drawdown_days_100,\n",
    "\n",
    "        # Extended analytics (no OBV fields)\n",
    "        \"log_prices\": log_price.values,\n",
    "        \"change_10dayret\": change_10dayret.values,\n",
    "        \"slope_over60_of_logprice\": slope_lp_recent60.values,\n",
    "        \"prior_slope_over60_of_logprice\": slope_lp_prev60.values,\n",
    "        \"60d_return_accel\": ret_accel_60.values,\n",
    "\n",
    "        \"750d_drawdown\": mdd_750.values,\n",
    "        \"750d_drawdownduration\": mdd_dur_750.values,\n",
    "\n",
    "        \"15d_downsidedeviation\": dd_15.values,\n",
    "        \"60d_downsidedeviation\": dd_60.values,\n",
    "        \"252d_downsidedeviation\": dd_252.values,\n",
    "\n",
    "        \"15d_upsidevolatility\": ud_15.values,\n",
    "        \"60d_upsidevolatility\": ud_60.values,\n",
    "        \"252d_upsidevolatility\": ud_252.values,\n",
    "\n",
    "        \"5d_volatility\": vol_5.values,\n",
    "        \"15d_volatility\": vol_15.values,\n",
    "        \"60d_volatility\": vol_60.values,\n",
    "        \"252d_volatility\": vol_252.values,\n",
    "\n",
    "        \"20d_parkinson_HL_volatility\": pk20.values,\n",
    "        \"5d_EMA_15dayvolatility\": ema5_of_vol15.values,\n",
    "\n",
    "        \"slope_over20_of_60d_volatility\": slope_vol60_over20.values,\n",
    "        \"slope_over60_of_252d_volatility\": slope_vol252_over60.values,\n",
    "\n",
    "        \"252d_dollar_volume_SMA\": dv_sma_252.values,\n",
    "        \"60d_dollar_volume_SMA\":  dv_sma_60.values,\n",
    "        \"252d_dollar_volume_accel\": dv252_accel_60.values,\n",
    "        \"60d_price_dollarVolume_correlation\": corr_px_dv_60.values,\n",
    "    }, index=g.index)\n",
    "\n",
    "        # Clean edge cases\n",
    "    out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Previously we were filling core fields with 0.0 (including returns & vol).\n",
    "    # Now we *only* zero-fill the handful of structural indicators\n",
    "    # where SQL compatibility requires it (like range positions, z-scores, drawdowns).\n",
    "    out[[\n",
    "        \"5_day_range_pos\",\"60_10_highlowrange_zscore\",\n",
    "        \"drawdown_percent\",\"drawdown_duration_days\"\n",
    "    ]] = out[[\n",
    "        \"5_day_range_pos\",\"60_10_highlowrange_zscore\",\n",
    "        \"drawdown_percent\",\"drawdown_duration_days\"\n",
    "    ]].fillna(0.0)\n",
    "\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------- Run for TARGET_DATE (from `combined`) --------------------\n",
    "_ = combined.copy()\n",
    "_.columns = [c.strip().lower() for c in _.columns]\n",
    "\n",
    "# Build a proper datetime column named 'date'\n",
    "if \"dt\" in _.columns and \"date\" not in _.columns:\n",
    "    _[\"date\"] = pd.to_datetime(_[\"dt\"])\n",
    "else:\n",
    "    _[\"date\"] = pd.to_datetime(_[\"date\"])\n",
    "\n",
    "for c in [\"open\",\"high\",\"low\",\"close\",\"adj_close\"]:\n",
    "    _[c] = pd.to_numeric(_[c], errors=\"coerce\")\n",
    "if \"volume\" in _.columns:\n",
    "    _[\"volume\"] = pd.to_numeric(_[\"volume\"], errors=\"coerce\")\n",
    "\n",
    "_ = _.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Compare on PURE DATE to avoid dtype/timezone issues\n",
    "T_DATE = pd.Timestamp(TARGET_DATE).date()\n",
    "\n",
    "daily_chunks = []\n",
    "for tkr, g in _.groupby(\"ticker\", sort=False):\n",
    "    # If this ticker has any row on the calendar date, keep it\n",
    "    if not (g[\"date\"].dt.date == T_DATE).any():\n",
    "        continue\n",
    "    all_rows = per_ticker_metrics_all_rows(g)\n",
    "    day_row = all_rows[all_rows[\"date\"].dt.date == T_DATE]\n",
    "    if not day_row.empty:\n",
    "        daily_chunks.append(day_row)\n",
    "\n",
    "daily_metrics = (\n",
    "    pd.concat(daily_chunks, ignore_index=True)\n",
    "      .sort_values([\"ticker\",\"date\"])\n",
    "      .reset_index(drop=True)\n",
    "    if daily_chunks else pd.DataFrame()\n",
    ")\n",
    "\n",
    "print(f\"Daily metrics rows for {T_DATE}: {len(daily_metrics):,}\")\n",
    "daily_metrics.head()\n",
    "\n",
    "num_rows = len(data)\n",
    "print(num_rows)\n",
    "\n",
    "#CHANGING DATA TO DATABASE FORMAT\n",
    "\n",
    "\n",
    "# You already have daily_metrics as a pandas DataFrame\n",
    "# Assumes it has at least: 'ticker' column, plus metric columns named by metric_code, and possibly 'date' etc.\n",
    "\n",
    "db_path = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "# --- Connect & load dimensions ---\n",
    "con = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "dim_ticker = con.execute(\"\"\"\n",
    "    SELECT ticker_id, ticker\n",
    "    FROM dim_ticker\n",
    "\"\"\").df()\n",
    "\n",
    "dim_metric = con.execute(\"\"\"\n",
    "    SELECT metric_id, metric_code\n",
    "    FROM dim_metric\n",
    "\"\"\").df()\n",
    "\n",
    "con.close()\n",
    "\n",
    "# --- Map ticker -> ticker_id ---\n",
    "ticker_map = dict(zip(dim_ticker[\"ticker\"], dim_ticker[\"ticker_id\"]))\n",
    "daily_metrics[\"ticker_id\"] = daily_metrics[\"ticker\"].map(ticker_map)\n",
    "\n",
    "# Optional: sanity check for unmapped tickers\n",
    "unmapped_tickers = daily_metrics.loc[daily_metrics[\"ticker_id\"].isna(), \"ticker\"].unique()\n",
    "if len(unmapped_tickers):\n",
    "    print(f\"[WARN] {len(unmapped_tickers)} tickers not found in dim_ticker:\", unmapped_tickers[:10], \"...\" if len(unmapped_tickers) > 10 else \"\")\n",
    "\n",
    "# Drop original ticker\n",
    "daily_metrics = daily_metrics.drop(columns=[\"ticker\"])\n",
    "\n",
    "# --- Rename metric_code columns -> metric_id ---\n",
    "metric_map = dict(zip(dim_metric[\"metric_code\"], dim_metric[\"metric_id\"]))\n",
    "\n",
    "# Build rename dict only for columns that match known metric_codes\n",
    "rename_dict = {col: metric_map[col] for col in daily_metrics.columns if col in metric_map}\n",
    "\n",
    "# Apply rename\n",
    "daily_metrics = daily_metrics.rename(columns=rename_dict)\n",
    "\n",
    "# Optional: report which metric columns could not be mapped\n",
    "metric_like_cols = [c for c in daily_metrics.columns if c not in {\"date\", \"ticker_id\"}]\n",
    "unmapped_metrics = [c for c in metric_like_cols if c not in metric_map.values()]  # after rename, mapped ones are metric_ids\n",
    "if unmapped_metrics:\n",
    "    print(f\"[INFO] {len(unmapped_metrics)} columns did not match any metric_code -> metric_id mapping (kept as-is):\")\n",
    "    print(unmapped_metrics[:20], \"...\" if len(unmapped_metrics) > 20 else \"\")\n",
    "\n",
    "# --- Show result ---\n",
    "print(daily_metrics.head())\n",
    "\n",
    "# SENDING THE DATA TO THE DUCK DATA BASE\n",
    "\n",
    "# --- Existing connection (adjust if needed) ---\n",
    "db_path = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# --- Assumes you already have a pandas DataFrame named `daily_metrics` in memory ---\n",
    "PRICE_COLS_ORDERED = [\"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
    "\n",
    "# 1) Normalize date column -> 'dt' (date type)\n",
    "df = daily_metrics.copy()\n",
    "if \"dt\" not in df.columns and \"date\" in df.columns:\n",
    "    df = df.rename(columns={\"date\": \"dt\"})\n",
    "if \"dt\" not in df.columns:\n",
    "    raise ValueError(\"daily_metrics must have a 'dt' or 'date' column.\")\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"]).dt.date\n",
    "\n",
    "# 2) Insert/Upsert into fact_price_daily\n",
    "present_price_cols = [c for c in PRICE_COLS_ORDERED if c in df.columns]\n",
    "missing_id_cols = [c for c in [\"ticker_id\"] if c not in df.columns]\n",
    "if missing_id_cols:\n",
    "    raise ValueError(f\"Missing required ID columns in daily_metrics: {missing_id_cols}\")\n",
    "\n",
    "if present_price_cols:\n",
    "    price_insert_cols = [\"ticker_id\", \"dt\"] + present_price_cols\n",
    "    df_price = df[price_insert_cols].copy()\n",
    "    con.register(\"df_price\", df_price)\n",
    "\n",
    "    set_clauses = \", \".join([f\"{c} = s.{c}\" for c in present_price_cols])  # no qualifier on left\n",
    "    insert_cols = \", \".join(price_insert_cols)\n",
    "    insert_vals = \", \".join([f\"s.{c}\" for c in price_insert_cols])\n",
    "\n",
    "    merge_sql_price = f\"\"\"\n",
    "    BEGIN TRANSACTION;\n",
    "\n",
    "    MERGE INTO fact_price_daily t\n",
    "    USING df_price s\n",
    "    ON t.ticker_id = s.ticker_id AND t.dt = s.dt\n",
    "    WHEN MATCHED THEN UPDATE SET {set_clauses}\n",
    "    WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals});\n",
    "\n",
    "    COMMIT;\n",
    "    \"\"\"\n",
    "    con.execute(merge_sql_price)\n",
    "else:\n",
    "    df_price = pd.DataFrame(columns=[\"ticker_id\",\"dt\"])  # for counts later\n",
    "\n",
    "# 3) Drop price columns from the working DataFrame\n",
    "df_no_price = df.drop(columns=[c for c in PRICE_COLS_ORDERED if c in df.columns])\n",
    "\n",
    "# 4) Unpivot metrics and upsert into fact_metric_daily\n",
    "NON_METRIC = set([\"ticker_id\", \"dt\"] + PRICE_COLS_ORDERED)\n",
    "metric_cols = [c for c in df_no_price.columns if c not in NON_METRIC]\n",
    "\n",
    "if metric_cols:\n",
    "    long_metrics = df_no_price.melt(\n",
    "        id_vars=[\"ticker_id\", \"dt\"],\n",
    "        value_vars=metric_cols,\n",
    "        var_name=\"metric_id\",\n",
    "        value_name=\"value\"\n",
    "    ).dropna(subset=[\"value\"])\n",
    "\n",
    "    # Optionally coerce numeric values\n",
    "    # long_metrics[\"value\"] = pd.to_numeric(long_metrics[\"value\"], errors=\"coerce\").dropna()\n",
    "\n",
    "    con.register(\"df_metrics_long\", long_metrics)\n",
    "\n",
    "    merge_sql_metrics = \"\"\"\n",
    "    BEGIN TRANSACTION;\n",
    "\n",
    "    MERGE INTO fact_metric_daily t\n",
    "    USING df_metrics_long s\n",
    "    ON t.ticker_id = s.ticker_id AND t.dt = s.dt AND t.metric_id = s.metric_id\n",
    "    WHEN MATCHED THEN UPDATE SET value = s.value\n",
    "    WHEN NOT MATCHED THEN INSERT (metric_id, dt, ticker_id, value)\n",
    "    VALUES (s.metric_id, s.dt, s.ticker_id, s.value);\n",
    "\n",
    "    COMMIT;\n",
    "    \"\"\"\n",
    "    con.execute(merge_sql_metrics)\n",
    "else:\n",
    "    long_metrics = pd.DataFrame(columns=[\"ticker_id\",\"dt\",\"metric_id\",\"value\"])\n",
    "\n",
    "# 5) Update your in-session daily_metrics without price columns\n",
    "daily_metrics = df_no_price\n",
    "\n",
    "# Quick peek\n",
    "print(\"Inserted/updated prices for rows:\", len(df_price))\n",
    "print(\"Inserted/updated metric rows:\", len(long_metrics))\n",
    "print(\"daily_metrics columns after dropping price fields:\", list(daily_metrics.columns))\n",
    "\n",
    "\n",
    "# 1) Ensure snapshot table exists (idempotent)\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS snapshot_metric_latest (\n",
    "  ticker_id INTEGER NOT NULL,\n",
    "  metric_id INTEGER NOT NULL,\n",
    "  dt        DATE    NOT NULL,\n",
    "  value     DOUBLE,\n",
    "  PRIMARY KEY (ticker_id, metric_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 2) Materialize latest rows WITHOUT window functions (no QUALIFY)\n",
    "#    We join fact_metric_daily to its per-key MAX(dt)\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TEMP TABLE _latest_rows AS\n",
    "WITH mx AS (\n",
    "  SELECT ticker_id, metric_id, MAX(dt) AS dt\n",
    "  FROM fact_metric_daily\n",
    "  GROUP BY 1,2\n",
    ")\n",
    "SELECT\n",
    "  f.ticker_id,\n",
    "  f.metric_id,\n",
    "  CAST(f.dt AS DATE) AS dt,\n",
    "  f.value\n",
    "FROM fact_metric_daily f\n",
    "JOIN mx\n",
    "  ON f.ticker_id = mx.ticker_id\n",
    " AND f.metric_id = mx.metric_id\n",
    " AND f.dt = mx.dt;\n",
    "\"\"\")\n",
    "\n",
    "# 3) Upsert into snapshot via MERGE\n",
    "con.execute(\"\"\"\n",
    "MERGE INTO snapshot_metric_latest AS t\n",
    "USING _latest_rows AS s\n",
    "ON t.ticker_id = s.ticker_id AND t.metric_id = s.metric_id\n",
    "WHEN MATCHED AND (t.dt <> s.dt OR (t.value IS DISTINCT FROM s.value)) THEN\n",
    "  UPDATE SET dt = s.dt, value = s.value\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (ticker_id, metric_id, dt, value)\n",
    "  VALUES (s.ticker_id, s.metric_id, s.dt, s.value);\n",
    "\"\"\")\n",
    "\n",
    "# 4) Optional: prune keys that no longer exist in fact_metric_daily\n",
    "con.execute(\"\"\"\n",
    "DELETE FROM snapshot_metric_latest t\n",
    "WHERE NOT EXISTS (\n",
    "  SELECT 1 FROM (\n",
    "    SELECT DISTINCT ticker_id, metric_id FROM fact_metric_daily\n",
    "  ) k\n",
    "  WHERE k.ticker_id = t.ticker_id AND k.metric_id = t.metric_id\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 5) Sanity check\n",
    "print(\"Rows in snapshot:\", con.execute(\"SELECT COUNT(*) FROM snapshot_metric_latest\").fetchone()[0])\n",
    "print(con.execute(\"\"\"\n",
    "  SELECT * FROM snapshot_metric_latest\n",
    "  ORDER BY ticker_id, metric_id\n",
    "  LIMIT 10\n",
    "\"\"\").fetchdf())\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61473116-55c4-44e8-af98-fc52d6f4a27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89080f-8075-467f-b6f9-c3ce68b6aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c826ad-dbb4-4e66-aec4-d04061c76376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\n",
      "  Using cached duckdb-1.4.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (14 kB)\n",
      "Using cached duckdb-1.4.0-cp311-cp311-macosx_10_9_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-1.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install duckdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e0bd6-7050-4841-85af-56661224733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
