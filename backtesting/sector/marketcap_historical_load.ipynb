{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7953ea4a-4f0d-4522-8395-054880b48bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float-shares history rows: 510454 for 492 tickers\n",
      "ticker         dt  shares_float  adj_close   market_cap\n",
      "     A 2025-10-08     282494000        NaN          NaN\n",
      "     A 2025-10-07     282494000     138.56 3.914237e+10\n",
      "     A 2025-10-06     282494000     141.61 4.000398e+10\n",
      "     A 2025-10-05     282494000        NaN          NaN\n",
      "     A 2025-10-04     282494000        NaN          NaN\n",
      "     A 2025-10-03     282494000     141.64 4.001245e+10\n",
      "     A 2025-10-02     282494000     138.70 3.918192e+10\n",
      "     A 2025-10-01     282494000     138.58 3.914802e+10\n",
      "  AAPL 2025-10-08   14814270914        NaN          NaN\n",
      "  AAPL 2025-10-07   14814270914     256.48 3.799564e+12\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import asyncio, aiohttp, math, sys\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# ==================== USER SETTINGS ====================\n",
    "DB_PATH     = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "API_KEY     = \"c5PobUQjaaMTHySILWqmWi9uyIDqYJBi\"\n",
    "BASE        = \"https://financialmodelingprep.com\"\n",
    "# Concurrency/pacing tuned for Starter (~300 req/min). One request per symbol.\n",
    "MAX_WORKERS = 6\n",
    "TIMEOUT_SEC = 20\n",
    "RETRIES     = 3\n",
    "BATCH_SLEEP_SEC = 5.0   # small pause between batches to smooth bursts\n",
    "BATCH_SIZE  = 50        # symbols per batch (<= MAX_WORKERS * ~8 is fine)\n",
    "\n",
    "# ==================== PREP TICKERS (Stocks only) ====================\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "tickers = [t[0] for t in con.execute(\"\"\"\n",
    "  SELECT ticker\n",
    "  FROM dim_ticker\n",
    "  WHERE UPPER(COALESCE(ticker_type,'')) = 'STOCK'\n",
    "\"\"\").fetchall()]\n",
    "con.close()\n",
    "\n",
    "if not tickers:\n",
    "    print(\"No tickers with ticker_type='Stock' found.\", file=sys.stderr)\n",
    "    sys.exit(0)\n",
    "\n",
    "# ==================== DATE WINDOW ====================\n",
    "today_utc = datetime.now(timezone.utc).date()\n",
    "from_dt   = today_utc - timedelta(days=365*3 + 5)   # pad a few days\n",
    "from_iso  = from_dt.isoformat()\n",
    "\n",
    "# ==================== HTTP HELPERS ====================\n",
    "sem = asyncio.Semaphore(MAX_WORKERS)\n",
    "\n",
    "async def fetch_json(session: aiohttp.ClientSession, url: str):\n",
    "    for attempt in range(RETRIES):\n",
    "        try:\n",
    "            async with sem:\n",
    "                async with session.get(url, timeout=TIMEOUT_SEC) as r:\n",
    "                    if r.status == 429:\n",
    "                        # brief backoff; progressive\n",
    "                        await asyncio.sleep(0.6 * (attempt + 1))\n",
    "                        continue\n",
    "                    if r.status == 200:\n",
    "                        return await r.json()\n",
    "                    # soft-fail for non-200\n",
    "                    return None\n",
    "        except Exception:\n",
    "            await asyncio.sleep(0.5 * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def parse_float_point(d: dict):\n",
    "    \"\"\"Robustly extract a float-shares value from a historical row.\"\"\"\n",
    "    for k in (\"floatShares\", \"sharesFloat\", \"freeFloat\", \"float\"):\n",
    "        v = d.get(k)\n",
    "        if v is not None:\n",
    "            try:\n",
    "                v = float(v)\n",
    "                if v > 0:\n",
    "                    return v\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "# ==================== PER-SYMBOL WORK ====================\n",
    "async def fetch_float_history(session: aiohttp.ClientSession, sym: str):\n",
    "    \"\"\"\n",
    "    Pull ~3y historical float shares for a single symbol.\n",
    "    Endpoint (historical): /api/v4/historical/shares_float?symbol=SYMB\n",
    "    \"\"\"\n",
    "    url = f\"{BASE}/api/v4/historical/shares_float?symbol={sym}&apikey={API_KEY}\"\n",
    "    payload = await fetch_json(session, url)\n",
    "    rows = []\n",
    "    if isinstance(payload, list) and payload:\n",
    "        for rec in payload:\n",
    "            dt_str = rec.get(\"date\")\n",
    "            if not dt_str:\n",
    "                continue\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(dt_str[:10]).date()\n",
    "            except Exception:\n",
    "                continue\n",
    "            if dt < from_dt:\n",
    "                continue\n",
    "            fs = parse_float_point(rec)\n",
    "            if fs is None:\n",
    "                continue\n",
    "            rows.append({\"ticker\": sym, \"dt\": dt, \"shares_float\": fs})\n",
    "    return rows\n",
    "\n",
    "# ==================== MAIN ASYNC ====================\n",
    "async def main() -> pd.DataFrame:\n",
    "    conn = aiohttp.TCPConnector(limit=None, ssl=False)\n",
    "    all_rows = []\n",
    "    async with aiohttp.ClientSession(connector=conn) as session:\n",
    "        for i in range(0, len(tickers), BATCH_SIZE):\n",
    "            batch = tickers[i:i+BATCH_SIZE]\n",
    "            batch_rows_list = await asyncio.gather(*[fetch_float_history(session, t) for t in batch])\n",
    "            for br in batch_rows_list:\n",
    "                if br:\n",
    "                    all_rows.extend(br)\n",
    "            # gentle pacing between batches to stay well under ~300/min budget\n",
    "            await asyncio.sleep(BATCH_SLEEP_SEC)\n",
    "    df = pd.DataFrame(all_rows, columns=[\"ticker\",\"dt\",\"shares_float\"])\n",
    "    if not df.empty:\n",
    "        df[\"ticker\"] = df[\"ticker\"].str.upper()\n",
    "        df[\"dt\"] = pd.to_datetime(df[\"dt\"]).dt.date\n",
    "        # De-dup in case endpoint returns dup dates\n",
    "        df = df.drop_duplicates(subset=[\"ticker\",\"dt\"], keep=\"last\")\n",
    "    print(f\"Float-shares history rows: {len(df)} for {df['ticker'].nunique() if not df.empty else 0} tickers\")\n",
    "    return df\n",
    "\n",
    "# ==================== RUN ====================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        loop = asyncio.get_running_loop()\n",
    "        df_hist = loop.run_until_complete(main())\n",
    "    except RuntimeError:\n",
    "        df_hist = asyncio.run(main())\n",
    "\n",
    "    if df_hist.empty:\n",
    "        print(\"No historical float-shares rows retrieved.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # ==================== UPSERT INTO DUCKDB ====================\n",
    "    # We compute free-float market cap per day when a matching price exists.\n",
    "    con = duckdb.connect(DB_PATH)\n",
    "    con.register(\"df_hist\", df_hist)\n",
    "\n",
    "    # 1) Create a temp prepared set joined to dim_ticker & price\n",
    "    #    Note: if fact_price_daily lacks a row for a given date (holiday/weekend),\n",
    "    #    that row wonâ€™t produce market_cap; we still store shares_float and adj_close (NULL).\n",
    "    sql_merge = \"\"\"\n",
    "    WITH src AS (\n",
    "      SELECT\n",
    "        UPPER(ticker) AS ticker,\n",
    "        CAST(dt AS DATE) AS dt,\n",
    "        CAST(shares_float AS DOUBLE) AS shares_float\n",
    "      FROM df_hist\n",
    "    ),\n",
    "    j AS (\n",
    "      SELECT\n",
    "        dtk.ticker_id,\n",
    "        s.dt,\n",
    "        /* Keep shares_outstanding NULL in this loader (we're loading float history).\n",
    "           You can augment later with a separate historical shares outstanding job if desired. */\n",
    "        NULL::DOUBLE AS shares_outstanding,\n",
    "        s.shares_float,\n",
    "        fpd.adj_close,\n",
    "        CASE WHEN fpd.adj_close IS NOT NULL THEN fpd.adj_close * s.shares_float ELSE NULL END AS market_cap\n",
    "      FROM src s\n",
    "      JOIN dim_ticker dtk ON dtk.ticker = s.ticker\n",
    "      LEFT JOIN fact_price_daily fpd\n",
    "        ON fpd.ticker_id = dtk.ticker_id\n",
    "       AND fpd.dt = s.dt\n",
    "    )\n",
    "    MERGE INTO fact_marketcap_daily AS t\n",
    "    USING j AS s\n",
    "    ON t.ticker_id = s.ticker_id AND t.dt = s.dt\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      shares_outstanding = COALESCE(s.shares_outstanding, t.shares_outstanding),\n",
    "      shares_float      = s.shares_float,\n",
    "      adj_close         = COALESCE(s.adj_close, t.adj_close),\n",
    "      market_cap        = COALESCE(s.market_cap, t.market_cap)\n",
    "    WHEN NOT MATCHED THEN INSERT (ticker_id, dt, shares_outstanding, shares_float, adj_close, market_cap)\n",
    "    VALUES (s.ticker_id, s.dt, s.shares_outstanding, s.shares_float, s.adj_close, s.market_cap);\n",
    "    \"\"\"\n",
    "    con.execute(sql_merge)\n",
    "\n",
    "    # 2) Optional: quick preview of most recent few days written\n",
    "    preview = con.execute(\"\"\"\n",
    "      SELECT d.ticker, t.dt, t.shares_float, t.adj_close, t.market_cap\n",
    "      FROM fact_marketcap_daily t\n",
    "      JOIN dim_ticker d ON d.ticker_id = t.ticker_id\n",
    "      WHERE t.dt >= CURRENT_DATE - INTERVAL 7 DAY\n",
    "        AND d.ticker IN (SELECT DISTINCT ticker FROM df_hist)\n",
    "      ORDER BY d.ticker, t.dt DESC\n",
    "      LIMIT 200\n",
    "    \"\"\").df()\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # Print a tiny sample to console\n",
    "    print(preview.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aad533f-0a46-4b56-b05a-5e4d07364c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST 10 ROWS (oldest) ===\n",
      " ticker_id         dt  shares_outstanding  shares_float  adj_close   market_cap\n",
      "       247 2022-10-04                <NA>     402130770      46.92 1.886798e+10\n",
      "       377 2022-10-04                <NA>      69048256     244.27 1.686642e+10\n",
      "       241 2022-10-04                <NA>    4101935060      26.35 1.080860e+11\n",
      "       236 2022-10-04                <NA>     555249883      90.51 5.025567e+10\n",
      "       239 2022-10-04                <NA>     229383541      88.46 2.029127e+10\n",
      "       244 2022-10-04                <NA>     360398632      29.00 1.045156e+10\n",
      "       245 2022-10-04                <NA>     389499326      23.99 9.344089e+09\n",
      "       242 2022-10-04                <NA>     273923967     403.90 1.106379e+11\n",
      "       410 2022-10-04                <NA>      52346888     198.23 1.037672e+10\n",
      "       273 2022-10-04                <NA>    1698036534        NaN          NaN\n",
      "\n",
      "=== LAST 10 ROWS (most recent) ===\n",
      " ticker_id         dt  shares_outstanding  shares_float  adj_close  market_cap\n",
      "       345 2025-10-08                <NA>     279497645        NaN         NaN\n",
      "       361 2025-10-08                <NA>    2191859284        NaN         NaN\n",
      "       366 2025-10-08                <NA>    2338108372        NaN         NaN\n",
      "       392 2025-10-08                <NA>     888069889        NaN         NaN\n",
      "       212 2025-10-08                <NA>     240510981        NaN         NaN\n",
      "       230 2025-10-08                <NA>     641243741        NaN         NaN\n",
      "       271 2025-10-08                <NA>    3870761363        NaN         NaN\n",
      "       288 2025-10-08                <NA>     467831959        NaN         NaN\n",
      "       289 2025-10-08                <NA>     321250373        NaN         NaN\n",
      "       295 2025-10-08                <NA>     237341018        NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "# --- First 10 rows (oldest) ---\n",
    "first10 = con.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM fact_marketcap_daily\n",
    "    ORDER BY dt ASC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "# --- Last 10 rows (most recent) ---\n",
    "last10 = con.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM fact_marketcap_daily\n",
    "    ORDER BY dt DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"=== FIRST 10 ROWS (oldest) ===\")\n",
    "print(first10.to_string(index=False))\n",
    "print(\"\\n=== LAST 10 ROWS (most recent) ===\")\n",
    "print(last10.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274511c-4db4-4ce8-a5ae-aabf59f7aa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
