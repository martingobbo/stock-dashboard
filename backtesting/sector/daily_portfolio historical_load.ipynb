{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087c3605-937c-4bea-8ef7-f7ce0b263958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     portfolio ticker          dt\n",
      "0      sector::Basic Materials    DOW  2024-10-24\n",
      "1         subsector::Chemicals    DOW  2024-10-24\n",
      "2          sector::Real Estate     WY  2024-10-24\n",
      "3  subsector::REIT - Specialty     WY  2024-10-24\n",
      "4   sector::Financial Services     RF  2024-10-24\n",
      "5  subsector::Banks - Regional     RF  2024-10-24\n",
      "6          sector::Real Estate    FRT  2024-10-24\n",
      "7     subsector::REIT - Retail    FRT  2024-10-24\n",
      "                                        portfolio ticker          dt\n",
      "251500                        sector::Industrials    XYL  2025-10-08\n",
      "251501          subsector::Industrial - Machinery    XYL  2025-10-08\n",
      "251502                 sector::Financial Services    IVZ  2025-10-08\n",
      "251503                subsector::Asset Management    IVZ  2025-10-08\n",
      "251504             sector::Communication Services   META  2025-10-08\n",
      "251505  subsector::Internet Content & Information   META  2025-10-08\n",
      "251506                  sector::Consumer Cyclical     GM  2025-10-08\n",
      "251507            subsector::Auto - Manufacturers     GM  2025-10-08\n",
      "rows: 251508 | unique dates: 250 | portfolios: 129\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import asyncio, aiohttp\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY   = \"c5PobUQjaaMTHySILWqmWi9uyIDqYJBi\"\n",
    "BASE      = \"https://financialmodelingprep.com/api/v3\"\n",
    "MAX_WORKERS, TIMEOUT_SEC, RETRIES = 5, 15, 3\n",
    "N_BUSINESS_DAYS = 250\n",
    "\n",
    "# ---------- Dates: strictly business days (Mon–Fri) ----------\n",
    "def last_business_days(end: date, periods: int) -> pd.DatetimeIndex:\n",
    "    # Mon–Fri only. If you want US market holidays too, replace with CustomBusinessDay(calendar=...)\n",
    "    return pd.bdate_range(end=pd.Timestamp(end), periods=periods, freq=\"B\")\n",
    "\n",
    "# ---------- HTTP ----------\n",
    "async def fetch_json(session, url, retries=RETRIES):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, timeout=TIMEOUT_SEC) as r:\n",
    "                if r.status == 200:\n",
    "                    return await r.json()\n",
    "                if r.status in (429, 500, 502, 503, 504):\n",
    "                    await asyncio.sleep(1.5 * (attempt + 1))\n",
    "                else:\n",
    "                    await asyncio.sleep(0.6)\n",
    "        except Exception:\n",
    "            await asyncio.sleep(0.6)\n",
    "    raise RuntimeError(f\"Failed GET {url}\")\n",
    "\n",
    "# ---------- Step 1: current members + change log ----------\n",
    "async def load_spx_members_and_changes():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        cur_url  = f\"{BASE}/sp500_constituent?apikey={API_KEY}\"\n",
    "        hist_url = f\"{BASE}/historical/sp500_constituent?apikey={API_KEY}\"\n",
    "        cur, hist = await asyncio.gather(\n",
    "            fetch_json(session, cur_url),\n",
    "            fetch_json(session, hist_url),\n",
    "        )\n",
    "    current = {r[\"symbol\"] for r in cur if isinstance(r, dict) and r.get(\"symbol\")}\n",
    "    chg = pd.DataFrame(hist)\n",
    "    if chg.empty:\n",
    "        chg = pd.DataFrame(columns=[\"date\", \"symbol\", \"addedSecurity\", \"removedTicker\"])\n",
    "    else:\n",
    "        keep = [c for c in [\"date\", \"symbol\", \"addedSecurity\", \"removedTicker\", \"reason\"] if c in chg.columns]\n",
    "        chg = chg[keep].copy()\n",
    "        chg[\"date\"] = pd.to_datetime(chg[\"date\"]).dt.date\n",
    "    return current, chg\n",
    "\n",
    "# ---------- Step 2: daily membership reconstruction (backward) ----------\n",
    "def rebuild_daily_membership(current_set, changes_df, bdays: pd.DatetimeIndex):\n",
    "    \"\"\"\n",
    "    Produces a dict: {date -> set(symbols)} for each business day in `bdays`.\n",
    "\n",
    "    Logic:\n",
    "      - Start at the most recent business day with today's *current_set*.\n",
    "      - For each day going backwards:\n",
    "          * Record today's membership.\n",
    "          * Then UNDO any events that occurred on that date to get the previous day's set:\n",
    "             - If a symbol was ADDED on D, it should NOT be in the set BEFORE D  -> remove it.\n",
    "             - If a symbol was REMOVED on D, it SHOULD be in the set BEFORE D   -> add it.\n",
    "    \"\"\"\n",
    "    bdays_desc = list(reversed(bdays))  # newest -> oldest\n",
    "    min_day, max_day = bdays.min().date(), bdays.max().date()\n",
    "\n",
    "    if not changes_df.empty:\n",
    "        ev = changes_df[(changes_df[\"date\"] >= min_day) & (changes_df[\"date\"] <= max_day)]\n",
    "    else:\n",
    "        ev = pd.DataFrame(columns=[\"date\", \"symbol\", \"addedSecurity\", \"removedTicker\"])\n",
    "    events_by_date = {d: df for d, df in ev.groupby(\"date\")}\n",
    "\n",
    "    members_map = {}\n",
    "    cur = set(current_set)\n",
    "\n",
    "    for dt_ts in bdays_desc:\n",
    "        d = dt_ts.date()\n",
    "        # 1) Record membership for this date (after events of this date have taken effect)\n",
    "        members_map[d] = set(cur)\n",
    "\n",
    "        # 2) Prepare for the previous (earlier) date: undo today's events\n",
    "        todays = events_by_date.get(d)\n",
    "        if todays is not None:\n",
    "            for _, r in todays.iterrows():\n",
    "                sym = r.get(\"symbol\")\n",
    "                if not sym:\n",
    "                    continue\n",
    "                if str(r.get(\"addedSecurity\", \"\")).strip():\n",
    "                    # Added on d -> before d it wasn't a member\n",
    "                    if sym in cur:\n",
    "                        cur.remove(sym)\n",
    "                if str(r.get(\"removedTicker\", \"\")).strip():\n",
    "                    # Removed on d -> before d it was still a member\n",
    "                    cur.add(sym)\n",
    "\n",
    "    return members_map  # keys = each business-day date\n",
    "\n",
    "# ---------- Step 3: classifications (sector/subsector) ----------\n",
    "async def fetch_classifications(symbols):\n",
    "    sem = asyncio.Semaphore(MAX_WORKERS)\n",
    "\n",
    "    async def one(session, sym):\n",
    "        url = f\"{BASE}/profile/{sym}?apikey={API_KEY}\"\n",
    "        for attempt in range(RETRIES):\n",
    "            try:\n",
    "                async with sem:\n",
    "                    async with session.get(url, timeout=TIMEOUT_SEC) as r:\n",
    "                        if r.status == 200:\n",
    "                            js = await r.json()\n",
    "                            row = (js[0] if isinstance(js, list) and js else {}) or {}\n",
    "                            sector    = (row.get(\"sector\") or \"\").strip() or \"Unknown\"\n",
    "                            subsector = (row.get(\"subSector\") or row.get(\"subsector\") or row.get(\"industry\") or \"\").strip() or \"Unknown\"\n",
    "                            return sym, sector, subsector\n",
    "                        if r.status in (429, 500, 502, 503, 504):\n",
    "                            await asyncio.sleep(1.25 * (attempt + 1))\n",
    "                        else:\n",
    "                            await asyncio.sleep(0.6)\n",
    "            except Exception:\n",
    "                await asyncio.sleep(0.6)\n",
    "        return sym, \"Unknown\", \"Unknown\"\n",
    "\n",
    "    out = {}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [asyncio.create_task(one(session, s)) for s in symbols]\n",
    "        for t in asyncio.as_completed(tasks):\n",
    "            sym, sector, subsector = await t\n",
    "            out[sym] = {\"sector\": sector, \"subsector\": subsector}\n",
    "    return out\n",
    "\n",
    "# ---------- Step 4: build daily sector/subsector portfolios ----------\n",
    "def build_portfolios(bdays, members_map, classes):\n",
    "    rows = []\n",
    "    for dt_ts in bdays:  # oldest -> newest for stable ordering\n",
    "        d = dt_ts.date()\n",
    "        for sym in members_map.get(d, ()):\n",
    "            cls = classes.get(sym, {})\n",
    "            sector    = cls.get(\"sector\", \"Unknown\") or \"Unknown\"\n",
    "            subsector = cls.get(\"subsector\", \"Unknown\") or \"Unknown\"\n",
    "            rows.append((\"sector::\"    + sector,    sym, d))\n",
    "            rows.append((\"subsector::\" + subsector, sym, d))\n",
    "    df = pd.DataFrame(rows, columns=[\"portfolio\", \"ticker\", \"dt\"])\n",
    "    return df\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "async def main():\n",
    "    # A) Dates: strictly business days (no weekends)\n",
    "    bdays = last_business_days(end=date.today(), periods=N_BUSINESS_DAYS)\n",
    "\n",
    "    # B) Current members + change log (FMP)\n",
    "    current_members, changes_df = await load_spx_members_and_changes()\n",
    "\n",
    "    # C) Daily membership reconstruction (uses current+changes)\n",
    "    daily_members = rebuild_daily_membership(current_members, changes_df, bdays)\n",
    "\n",
    "    # D) Universe = all symbols that appeared on any of these days\n",
    "    all_syms = sorted({s for sset in daily_members.values() for s in sset})\n",
    "\n",
    "    # E) Fetch GICS sector/subsector once per symbol\n",
    "    classes = await fetch_classifications(all_syms)\n",
    "\n",
    "    # F) Build the daily portfolio rows FROM the reconstructed daily constituents\n",
    "    daily_sector_subsector_portfolios = build_portfolios(bdays, daily_members, classes)\n",
    "\n",
    "    print(daily_sector_subsector_portfolios.head(8))\n",
    "    print(daily_sector_subsector_portfolios.tail(8))\n",
    "    print(\n",
    "        \"rows:\", len(daily_sector_subsector_portfolios),\n",
    "        \"| unique dates:\", daily_sector_subsector_portfolios[\"dt\"].nunique(),\n",
    "        \"| portfolios:\", daily_sector_subsector_portfolios[\"portfolio\"].nunique()\n",
    "    )\n",
    "    return daily_sector_subsector_portfolios\n",
    "\n",
    "# ---------- RUNNER that works in both scripts and notebooks ----------\n",
    "def run_async(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = None\n",
    "\n",
    "    if loop and loop.is_running():\n",
    "        # Notebook/REPL path\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "        except ImportError as e:\n",
    "            raise RuntimeError(\n",
    "                \"An event loop is already running (notebook). \"\n",
    "                \"Either: (a) run `await main()` in a cell, or \"\n",
    "                \"(b) `pip install nest_asyncio` and retry.\"\n",
    "            ) from e\n",
    "        return loop.run_until_complete(coro)\n",
    "    else:\n",
    "        # Regular script path\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "# ===== Execute =====\n",
    "if __name__ == \"__main__\":\n",
    "    daily_sector_subsector_portfolios = run_async(main())\n",
    "else:\n",
    "    # If you're in a Jupyter cell, you can also do:\n",
    "    # daily_sector_subsector_portfolios = await main()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b714028e-97f6-45c5-a292-d96324f2f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMP universe: 504 | dim_ticker: 514 | missing: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BF-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRK-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IBKR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker\n",
       "0   AMTM\n",
       "1    APP\n",
       "2   BF-B\n",
       "3  BRK-B\n",
       "4    EME\n",
       "5   HOOD\n",
       "6   IBKR"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# --- Config ---\n",
    "DB_PATH = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "# 1) Universe from the portfolios DF (FMP S&P constituents over 250 business days)\n",
    "fmp_syms = (\n",
    "    daily_sector_subsector_portfolios[\"ticker\"]\n",
    "    .dropna().astype(str).str.strip().str.upper().unique()\n",
    ")\n",
    "fmp_set = set(fmp_syms)\n",
    "\n",
    "# 2) Read dim_ticker tickers\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "dim = con.execute(\"SELECT UPPER(TRIM(ticker)) AS ticker FROM dim_ticker\").df()\n",
    "con.close()\n",
    "dim_set = set(dim[\"ticker\"].dropna().astype(str))\n",
    "\n",
    "# 3) Missing = in FMP set but not in dim_ticker\n",
    "missing = sorted(fmp_set - dim_set)\n",
    "\n",
    "print(\n",
    "    f\"FMP universe: {len(fmp_set)} | \"\n",
    "    f\"dim_ticker: {len(dim_set)} | \"\n",
    "    f\"missing: {len(missing)}\"\n",
    ")\n",
    "\n",
    "missing_df = pd.DataFrame({\"ticker\": missing})\n",
    "display(missing_df.head(50))  # preview first 50\n",
    "# If you want the full list:\n",
    "# display(missing_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55351fa-d365-4a46-b7f4-7b875db626be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fcc8e-6811-4bdd-b4ab-a25fd75a76aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53dd951a-c337-446c-8d95-8e0b9874c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/strings/object_array.py:354: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  f = lambda x: new_pat.split(x, maxsplit=n)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_portfolio total rows: 129\n",
      "Computed weights rows: 231950 | unique dates: 238 | portfolios: 128\n",
      "   rows_written  days  portfolios\n",
      "0        231950   238         128\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Build sector/subsector portfolios and populate fact_portfolio_daily ===\n",
    "import re\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# ---- CONFIG: point to your DuckDB file ----\n",
    "DB_PATH = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "# ---- 1) Normalize portfolio codes (sector_technology, subsector_chemicals_technology) ----\n",
    "def slugify(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[^0-9a-z]+\", \"_\", s)\n",
    "    return s.strip(\"_\") or \"unknown\"\n",
    "\n",
    "# daily_sector_subsector_portfolios has rows like:\n",
    "#   portfolio: \"sector::Technology\" or \"subsector::Chemicals\"\n",
    "#   ticker: \"AAPL\"\n",
    "#   dt: date\n",
    "df = daily_sector_subsector_portfolios.copy()\n",
    "\n",
    "# Split kind/name\n",
    "tmp = df.copy()\n",
    "tmp[\"kind\"] = tmp[\"portfolio\"].str.split(\"::\", n=1).str[0].str.lower()       # 'sector' or 'subsector'\n",
    "tmp[\"name\"] = tmp[\"portfolio\"].str.split(\"::\", n=1).str[1].fillna(\"Unknown\")\n",
    "\n",
    "# Pivot to get both sector and subsector for each (dt, ticker)\n",
    "pivot = (\n",
    "    tmp\n",
    "    .pivot_table(index=[\"dt\", \"ticker\"], columns=\"kind\", values=\"name\", aggfunc=\"first\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Build assignment rows: two portfolios per (dt, ticker)\n",
    "assign_rows = []\n",
    "for r in pivot.itertuples(index=False):\n",
    "    dt, ticker = r.dt, r.ticker\n",
    "    sector_name    = getattr(r, \"sector\", \"Unknown\") or \"Unknown\"\n",
    "    subsector_name = getattr(r, \"subsector\", \"Unknown\") or \"Unknown\"\n",
    "\n",
    "    # sector code: sector_<sector>\n",
    "    sector_code = f\"sector_{slugify(sector_name)}\"\n",
    "    assign_rows.append((pd.to_datetime(dt).date(), ticker, sector_code))\n",
    "\n",
    "    # subsector code: subsector_<subsector>_<sector>\n",
    "    subsector_code = f\"subsector_{slugify(subsector_name)}_{slugify(sector_name)}\"\n",
    "    assign_rows.append((pd.to_datetime(dt).date(), ticker, subsector_code))\n",
    "\n",
    "assign = pd.DataFrame(assign_rows, columns=[\"dt\", \"ticker\", \"portfolio_code\"]).drop_duplicates()\n",
    "\n",
    "# ---- 2) Open DB, ensure tables exist ----\n",
    "con = duckdb.connect(DB_PATH)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dim_portfolio (\n",
    "  portfolio_id   INTEGER PRIMARY KEY,\n",
    "  portfolio_code TEXT UNIQUE NOT NULL\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create with the corrected column name; do NOT include market_cap.\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_portfolio_daily (\n",
    "  portfolio_id  INTEGER NOT NULL,\n",
    "  ticker_id     INTEGER NOT NULL,\n",
    "  dt            DATE    NOT NULL,\n",
    "  weight        DOUBLE,\n",
    "  market_value  DOUBLE,   -- holds the ticker's market cap for that day\n",
    "  PRIMARY KEY (portfolio_id, ticker_id, dt)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# --- Schema migration (if an older column named market_cap exists, drop it) ---\n",
    "cols_df = con.execute(\"PRAGMA table_info('fact_portfolio_daily')\").df()\n",
    "if \"market_cap\" in [c.lower() for c in cols_df[\"name\"].tolist()]:\n",
    "    con.execute(\"ALTER TABLE fact_portfolio_daily DROP COLUMN market_cap;\")\n",
    "\n",
    "# ---- 3) Ensure all portfolio_codes exist in dim_portfolio (assign new ids for new codes) ----\n",
    "existing_df = con.execute(\"SELECT portfolio_code, portfolio_id FROM dim_portfolio\").df()\n",
    "existing_codes = set(existing_df[\"portfolio_code\"]) if not existing_df.empty else set()\n",
    "needed_codes = sorted(set(assign[\"portfolio_code\"]) - existing_codes)\n",
    "\n",
    "if needed_codes:\n",
    "    max_id = 0 if existing_df.empty else int(existing_df[\"portfolio_id\"].max())\n",
    "    to_insert = [(max_id + i + 1, code) for i, code in enumerate(needed_codes)]\n",
    "    con.executemany(\"INSERT INTO dim_portfolio (portfolio_id, portfolio_code) VALUES (?, ?)\", to_insert)\n",
    "\n",
    "# (Optional) sanity check\n",
    "new_dim_count = con.execute(\"SELECT COUNT(*) FROM dim_portfolio\").fetchone()[0]\n",
    "print(f\"dim_portfolio total rows: {new_dim_count}\")\n",
    "\n",
    "# ---- 4) Register helper DataFrames and build weights from market caps ----\n",
    "# Map ticker -> ticker_id\n",
    "ticker_map = con.execute(\"SELECT ticker, ticker_id FROM dim_ticker\").df()\n",
    "\n",
    "# Register temp views\n",
    "con.register(\"assign\", assign)\n",
    "con.register(\"ticker_map\", ticker_map)\n",
    "\n",
    "# We’ll compute:\n",
    "# - portfolio_id for each assignment (join dim_portfolio)\n",
    "# - bring in fact_marketcap_daily for each (dt, ticker_id)\n",
    "# - total per portfolio_id/day\n",
    "# - weight = ticker_cap / total_cap\n",
    "sql_weights = \"\"\"\n",
    "WITH a AS (\n",
    "  SELECT \n",
    "    a.dt,\n",
    "    lower(a.ticker) AS ticker_lc,\n",
    "    a.portfolio_code\n",
    "  FROM assign a\n",
    "),\n",
    "a2 AS (\n",
    "  SELECT \n",
    "    a.dt,\n",
    "    tm.ticker_id,\n",
    "    dp.portfolio_id\n",
    "  FROM a\n",
    "  JOIN ticker_map tm\n",
    "    ON lower(tm.ticker) = a.ticker_lc\n",
    "  JOIN dim_portfolio dp\n",
    "    ON dp.portfolio_code = a.portfolio_code\n",
    "),\n",
    "m AS (\n",
    "  SELECT \n",
    "    a2.portfolio_id,\n",
    "    a2.ticker_id,\n",
    "    f.dt,\n",
    "    f.market_cap\n",
    "  FROM a2\n",
    "  JOIN fact_marketcap_daily f\n",
    "    ON f.ticker_id = a2.ticker_id\n",
    "   AND f.dt = a2.dt\n",
    "  WHERE f.market_cap IS NOT NULL\n",
    "),\n",
    "tot AS (\n",
    "  SELECT portfolio_id, dt, SUM(market_cap) AS total_cap\n",
    "  FROM m\n",
    "  GROUP BY portfolio_id, dt\n",
    "),\n",
    "weights AS (\n",
    "  SELECT\n",
    "    m.portfolio_id,\n",
    "    m.ticker_id,\n",
    "    m.dt,\n",
    "    m.market_cap,\n",
    "    CASE WHEN tot.total_cap > 0 THEN m.market_cap / tot.total_cap ELSE NULL END AS weight\n",
    "  FROM m\n",
    "  JOIN tot\n",
    "    ON tot.portfolio_id = m.portfolio_id AND tot.dt = m.dt\n",
    ")\n",
    "SELECT * FROM weights\n",
    "\"\"\"\n",
    "\n",
    "weights_df = con.execute(sql_weights).df()\n",
    "print(\n",
    "    f\"Computed weights rows: {len(weights_df)} | \"\n",
    "    f\"unique dates: {weights_df['dt'].nunique() if not weights_df.empty else 0} | \"\n",
    "    f\"portfolios: {weights_df['portfolio_id'].nunique() if not weights_df.empty else 0}\"\n",
    ")\n",
    "\n",
    "# ---- 5) Upsert into fact_portfolio_daily ----\n",
    "# IMPORTANT: market_value := ticker's market_cap for that day\n",
    "con.register(\"weights_df\", weights_df)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "MERGE INTO fact_portfolio_daily AS dst\n",
    "USING (\n",
    "  SELECT \n",
    "    portfolio_id, ticker_id, dt, \n",
    "    weight,\n",
    "    market_cap AS market_value\n",
    "  FROM weights_df\n",
    ") AS src\n",
    "ON  dst.portfolio_id = src.portfolio_id\n",
    "AND dst.ticker_id    = src.ticker_id\n",
    "AND dst.dt           = src.dt\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  weight       = src.weight,\n",
    "  market_value = src.market_value\n",
    "WHEN NOT MATCHED THEN INSERT\n",
    "  (portfolio_id, ticker_id, dt, weight, market_value)\n",
    "VALUES\n",
    "  (src.portfolio_id, src.ticker_id, src.dt, src.weight, src.market_value);\n",
    "\"\"\")\n",
    "\n",
    "# Small summary\n",
    "summary = con.execute(\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) AS rows_written,\n",
    "  COUNT(DISTINCT dt) AS days,\n",
    "  COUNT(DISTINCT portfolio_id) AS portfolios\n",
    "FROM fact_portfolio_daily\n",
    "WHERE dt IN (SELECT DISTINCT dt FROM weights_df)\n",
    "\"\"\").df()\n",
    "print(summary)\n",
    "\n",
    "# Clean up registered views\n",
    "con.unregister(\"assign\")\n",
    "con.unregister(\"ticker_map\")\n",
    "con.unregister(\"weights_df\")\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393c8be-d305-4fa5-a117-dd4223e50492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd140b-4668-4e71-b4ac-16eb6ebd7d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418846e-38d4-40af-86a9-f7973a5381bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fc1c1-3cf5-462a-b77d-de9072ac0911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8dcc6-bbbd-4cf9-be46-6051b65a1e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c27a20-020c-47db-bdc1-bc33217764c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coverage check (last 250 trading days; only ticker_type = 'Stock') ===\n",
      "Total (price universe) rows checked: 86,290\n",
      "Missing in fact_marketcap_daily: 1,868\n",
      "Missing in fact_portfolio_daily: 2,523\n",
      "Duplicates in fact_marketcap_daily (count > 1): 0\n",
      "Duplicates in fact_portfolio_daily (count > 1): 83,767\n",
      "\n",
      "--- Missing in fact_marketcap_daily: (dt, ticker) (showing up to 20) ---\n",
      "        dt ticker  ticker_id\n",
      "2025-01-31   BF.B         57\n",
      "2025-01-31  BRK.B         67\n",
      "2025-01-31    RTX        401\n",
      "2025-01-31   RVTY        402\n",
      "2025-01-31   SBAC        403\n",
      "2025-01-31   SBUX        404\n",
      "2025-01-31   SCHW        405\n",
      "2025-01-31    SHW        406\n",
      "2025-01-31    SLB        408\n",
      "2025-01-31   SMCI        409\n",
      "2025-01-31   SNPS        411\n",
      "2025-02-03   BF.B         57\n",
      "2025-02-03  BRK.B         67\n",
      "2025-02-03    RTX        401\n",
      "2025-02-03   RVTY        402\n",
      "2025-02-03   SBAC        403\n",
      "2025-02-03   SBUX        404\n",
      "2025-02-03   SCHW        405\n",
      "2025-02-03    SHW        406\n",
      "2025-02-03    SLB        408\n",
      "\n",
      "--- Missing in fact_portfolio_daily: (dt, ticker) (showing up to 20) ---\n",
      "        dt ticker  ticker_id\n",
      "2025-01-31   BF.B         57\n",
      "2025-01-31  BRK.B         67\n",
      "2025-01-31    CZR        123\n",
      "2025-01-31   ENPH        161\n",
      "2025-01-31   MKTX        308\n",
      "2025-01-31    RTX        401\n",
      "2025-01-31   RVTY        402\n",
      "2025-01-31   SBAC        403\n",
      "2025-01-31   SBUX        404\n",
      "2025-01-31   SCHW        405\n",
      "2025-01-31    SHW        406\n",
      "2025-01-31    SLB        408\n",
      "2025-01-31   SMCI        409\n",
      "2025-01-31   SNPS        411\n",
      "2025-01-31    WBA        480\n",
      "2025-02-03   BF.B         57\n",
      "2025-02-03  BRK.B         67\n",
      "2025-02-03    CZR        123\n",
      "2025-02-03   ENPH        161\n",
      "2025-02-03   MKTX        308\n",
      "\n",
      "--- Duplicates in fact_marketcap_daily: (dt, ticker, n_mc) (showing up to 20) ---\n",
      "None ✅\n",
      "\n",
      "--- Duplicates in fact_portfolio_daily: (dt, ticker, n_pf) (showing up to 20) ---\n",
      "        dt ticker  ticker_id  n_pf\n",
      "2025-01-31      A          1     2\n",
      "2025-01-31   AAPL          2     2\n",
      "2025-01-31   ABBV          3     2\n",
      "2025-01-31   ABNB          4     2\n",
      "2025-01-31    ABT          5     2\n",
      "2025-01-31   ACGL          6     2\n",
      "2025-01-31    ACN          7     2\n",
      "2025-01-31   ADBE          8     2\n",
      "2025-01-31    ADI          9     2\n",
      "2025-01-31    ADM         10     2\n",
      "2025-01-31    ADP         11     2\n",
      "2025-01-31   ADSK         12     2\n",
      "2025-01-31    AEE         13     2\n",
      "2025-01-31    AEP         14     2\n",
      "2025-01-31    AES         15     2\n",
      "2025-01-31    AFL         16     2\n",
      "2025-01-31    AIG         17     2\n",
      "2025-01-31    AIZ         18     2\n",
      "2025-01-31    AJG         19     2\n",
      "2025-01-31   AKAM         20     2\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3 (fixed): Inspect completeness across price, marketcap, and portfolio tables (last 250 days) ===\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = \"/Users/martingobbo/stock-dashboard/data/serving/analytics.duckdb\"\n",
    "\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "sql = \"\"\"\n",
    "-- Universe: only Stocks, last 250 trading days based on price table's max date\n",
    "WITH stocks AS (\n",
    "  SELECT ticker_id, ticker\n",
    "  FROM dim_ticker\n",
    "  WHERE lower(ticker_type) = 'stock'\n",
    "),\n",
    "px_all AS (\n",
    "  SELECT p.dt, p.ticker_id\n",
    "  FROM fact_price_daily p\n",
    "  JOIN stocks s USING (ticker_id)\n",
    "),\n",
    "maxd AS (\n",
    "  SELECT max(dt) AS max_dt FROM px_all\n",
    "),\n",
    "px AS (\n",
    "  SELECT *\n",
    "  FROM px_all\n",
    "  WHERE dt >= (SELECT max_dt FROM maxd) - INTERVAL 249 DAY\n",
    "),\n",
    "\n",
    "-- Collapse marketcap & portfolio to (dt, ticker_id, n_rows) so we can detect missing/dupes\n",
    "mc AS (\n",
    "  SELECT dt, ticker_id, COUNT(*) AS n_mc\n",
    "  FROM fact_marketcap_daily\n",
    "  GROUP BY 1,2\n",
    "),\n",
    "pf AS (\n",
    "  SELECT dt, ticker_id, COUNT(*) AS n_pf\n",
    "  FROM fact_portfolio_daily\n",
    "  GROUP BY 1,2\n",
    "),\n",
    "\n",
    "-- Bring tickers for readability\n",
    "px_labeled AS (\n",
    "  SELECT px.dt, px.ticker_id, s.ticker\n",
    "  FROM px\n",
    "  JOIN stocks s USING (ticker_id)\n",
    "),\n",
    "\n",
    "-- Join price universe to marketcap & portfolio aggregates\n",
    "joined AS (\n",
    "  SELECT\n",
    "    p.dt,\n",
    "    p.ticker_id,\n",
    "    p.ticker,\n",
    "    COALESCE(mc.n_mc, 0) AS n_mc,\n",
    "    COALESCE(pf.n_pf, 0) AS n_pf\n",
    "  FROM px_labeled p\n",
    "  LEFT JOIN mc ON mc.dt = p.dt AND mc.ticker_id = p.ticker_id\n",
    "  LEFT JOIN pf ON pf.dt = p.dt AND pf.ticker_id = p.ticker_id\n",
    ")\n",
    "\n",
    "SELECT * FROM joined\n",
    "ORDER BY dt, ticker;\n",
    "\"\"\"\n",
    "\n",
    "joined = con.execute(sql).df()\n",
    "\n",
    "# --- Missing sets: present in price but missing in marketcap/portfolio (n == 0)\n",
    "missing_mc = joined.loc[joined[\"n_mc\"] == 0, [\"dt\", \"ticker\", \"ticker_id\"]].copy()\n",
    "missing_pf = joined.loc[joined[\"n_pf\"] == 0, [\"dt\", \"ticker\", \"ticker_id\"]].copy()\n",
    "\n",
    "# --- Duplicates: combos where count != 1 (0 already captured as missing; here we show >1)\n",
    "dupes_mc = joined.loc[joined[\"n_mc\"] > 1, [\"dt\", \"ticker\", \"ticker_id\", \"n_mc\"]].copy()\n",
    "dupes_pf = joined.loc[joined[\"n_pf\"] > 1, [\"dt\", \"ticker\", \"ticker_id\", \"n_pf\"]].copy()\n",
    "\n",
    "# --- Quick summaries\n",
    "print(\"=== Coverage check (last 250 trading days; only ticker_type = 'Stock') ===\")\n",
    "print(f\"Total (price universe) rows checked: {len(joined):,}\")\n",
    "print(f\"Missing in fact_marketcap_daily: {len(missing_mc):,}\")\n",
    "print(f\"Missing in fact_portfolio_daily: {len(missing_pf):,}\")\n",
    "print(f\"Duplicates in fact_marketcap_daily (count > 1): {len(dupes_mc):,}\")\n",
    "print(f\"Duplicates in fact_portfolio_daily (count > 1): {len(dupes_pf):,}\")\n",
    "\n",
    "# --- Show a few samples for each issue (keep prints manageable)\n",
    "def preview(df, title, n=20):\n",
    "    print(f\"\\n--- {title} (showing up to {n}) ---\")\n",
    "    if df.empty:\n",
    "        print(\"None ✅\")\n",
    "    else:\n",
    "        print(df.sort_values([\"dt\",\"ticker\"]).head(n).to_string(index=False))\n",
    "\n",
    "preview(missing_mc, \"Missing in fact_marketcap_daily: (dt, ticker)\")\n",
    "preview(missing_pf, \"Missing in fact_portfolio_daily: (dt, ticker)\")\n",
    "preview(dupes_mc,   \"Duplicates in fact_marketcap_daily: (dt, ticker, n_mc)\")\n",
    "preview(dupes_pf,   \"Duplicates in fact_portfolio_daily: (dt, ticker, n_pf)\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4820df-1655-424c-9f7d-5d67e3a87a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
